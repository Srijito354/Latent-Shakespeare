{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_lines = open(\"input.txt\", \"r\")\n",
    "tokenized_lines = tokenized_lines.readlines()\n",
    "\n",
    "vocab = set()\n",
    "special_tokens = [\"<pad>\", \"<start>\", \"<end>\"]\n",
    "for sentence in tokenized_lines:\n",
    "    vocab.update(sentence.split())\n",
    "vocab = special_tokens + list(vocab)\n",
    "\n",
    "vocab_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "vocab_size = len(vocab)\n",
    "#print(vocab)\n",
    "#print(\"Vocab size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "PAD_IDX = vocab_to_index[PAD_TOKEN]\n",
    "\n",
    "def collate_batch(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "\n",
    "    #inputs = [torch.tensor(seq, dtype = torch.long()) for seq in inputs]\n",
    "    #targets = [torch.tensor(seq, dtype = torch.long()) for seq in targets]\n",
    "\n",
    "    padded_inputs = pad_sequence(inputs, batch_first=True, padding_value=PAD_IDX)\n",
    "    padded_targets = pad_sequence(targets, batch_first=True, padding_value=PAD_IDX)\n",
    "\n",
    "    return padded_inputs, padded_targets\n",
    "\n",
    "\n",
    "# 1. Rebuild vocab from lowercased text and include <unk>\n",
    "special_tokens = [\"<pad>\", \"<start>\", \"<end>\", \"<unk>\"]\n",
    "\n",
    "vocab_to_index = {}\n",
    "\n",
    "vocab = set()\n",
    "for sentence in tokenized_lines:\n",
    "    vocab.update(sentence.lower().split())      # lowercase here\n",
    "\n",
    "vocab = special_tokens + sorted(vocab)          # sorted for reproducibility\n",
    "vocab_to_index = {w:i for i,w in enumerate(vocab)}\n",
    "\n",
    "PAD_IDX = vocab_to_index[\"<pad>\"]\n",
    "UNK_IDX = vocab_to_index[\"<unk>\"]\n",
    "\n",
    "# 2. Update your Dataset to use .get(â€¦, UNK_IDX) instead of direct indexing\n",
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, tokenized_lines, vocab_to_idx):\n",
    "        self.data = [\n",
    "            line.lower().split()\n",
    "            for line in tokenized_lines\n",
    "            if len(line.lower().split()) > 2  # ignore short lines\n",
    "        ]\n",
    "        self.vocab_to_idx = vocab_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        words = self.data[idx]\n",
    "\n",
    "        # THIS should raise error if token is missing\n",
    "        input_ids = [self.vocab_to_idx.get(word, self.vocab_to_idx[\"<unk>\"]) for word in words[:-1]]\n",
    "        target_ids = [self.vocab_to_idx.get(word, self.vocab_to_idx[\"<unk>\"]) for word in words[1:]]\n",
    "\n",
    "        return torch.tensor(input_ids, dtype=torch.long), torch.tensor(target_ids, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndata = [\\n    line.lower().split()\\n    for line in tokenized_lines\\n    if len(line.lower().split()) > 2  # ignore short lines\\n]\\nfor i in range(5):\\n    words = data[i]\\n    print(words[:-1])\\n    print(words[1:])\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "data = [\n",
    "    line.lower().split()\n",
    "    for line in tokenized_lines\n",
    "    if len(line.lower().split()) > 2  # ignore short lines\n",
    "]\n",
    "for i in range(5):\n",
    "    words = data[i]\n",
    "    print(words[:-1])\n",
    "    print(words[1:])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encodings(seq_len, embedding_dim, device):\n",
    "    position = torch.arange(seq_len, dtype=torch.float, device=device).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, embedding_dim, 2, device=device).float() * (-math.log(10000.0) / embedding_dim))\n",
    "    pe = torch.zeros(seq_len, embedding_dim, device=device)\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nclass self_attention(nn.Module):\\n    def __init__(self):\\n        super(self_attention, self).__init__()\\n        #self.w_qkv_downscale = nn.Linear(in_channels=16, out_channels=2)\\n        #self.latent_upscale = nn.Linear(in_channels=2, out_channels=16)\\n        #self.layer_norm = nn.LayerNorm()\\n        self.softmax = nn.Softmax()\\n\\n    def forward(self, Q, K, V):\\n        #Q = self.w_qkv_downscale(Q)\\n        #K = self.w_qkv_downscale(K)\\n        #V = self.w_qkv_downscale(V)\\n        seq_len = Q.size(1)\\n        mask = torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1)\\n        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (16 ** 0.5) + mask\\n        #attention_weights = self.softmax(attention_scores, dim=-1)\\n        attention_weights = self.softmax(attention_scores)\\n        context = torch.matmul(attention_weights, V)\\n\\n        #context = self.latent_upscale(context)\\n\\n        # Residual + Norm\\n        # x = self.layer_norm(context + x)\\n\\n        # Feedforward + Norm\\n        #ff_out = self.feed_fwd(x)\\n        #out = self.layer_norm(ff_out + x)\\n\\n        # Final linear (optional)\\n        #return self.output_proj(out)\\n        return context\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "class self_attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(self_attention, self).__init__()\n",
    "        #self.w_qkv_downscale = nn.Linear(in_channels=16, out_channels=2)\n",
    "        #self.latent_upscale = nn.Linear(in_channels=2, out_channels=16)\n",
    "        #self.layer_norm = nn.LayerNorm()\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        #Q = self.w_qkv_downscale(Q)\n",
    "        #K = self.w_qkv_downscale(K)\n",
    "        #V = self.w_qkv_downscale(V)\n",
    "        seq_len = Q.size(1)\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1)\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (16 ** 0.5) + mask\n",
    "        #attention_weights = self.softmax(attention_scores, dim=-1)\n",
    "        attention_weights = self.softmax(attention_scores)\n",
    "        context = torch.matmul(attention_weights, V)\n",
    "\n",
    "        #context = self.latent_upscale(context)\n",
    "\n",
    "        # Residual + Norm\n",
    "        # x = self.layer_norm(context + x)\n",
    "\n",
    "        # Feedforward + Norm\n",
    "        #ff_out = self.feed_fwd(x)\n",
    "        #out = self.layer_norm(ff_out + x)\n",
    "\n",
    "        # Final linear (optional)\n",
    "        #return self.output_proj(out)\n",
    "        return context\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(self_attention, self).__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask=None):\n",
    "        # Q, K, V shape: (batch, seq_len, dim)\n",
    "        batch_size, seq_len, dim = Q.size()\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(dim)  # (batch, seq_len, seq_len)\n",
    "\n",
    "        # Causal mask (upper triangular)\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=Q.device) * float('-inf'), diagonal=1)\n",
    "        scores = scores + causal_mask\n",
    "\n",
    "        # Padding mask (optional)\n",
    "        if attn_mask is not None:\n",
    "            # attn_mask: (batch, 1, seq_len), 1 for keep, 0 for mask\n",
    "            scores = scores.masked_fill(attn_mask == 0, float('-inf'))\n",
    "\n",
    "        weights = self.softmax(scores)\n",
    "        context = torch.matmul(weights, V)  # (batch, seq_len, dim)\n",
    "\n",
    "        return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass Model(nn.Module):\\n    def __init__(self):\\n        super(Model, self).__init__()\\n        \\n        # creating the multi-headed attention block.\\n        self.self_attn1 = self_attention()\\n        self.self_attn2 = self_attention()\\n        self.self_attn3 = self_attention()\\n        self.self_attn4 = self_attention()\\n\\n        self.self_attn5 = self_attention()\\n        self.self_attn6 = self_attention()\\n        self.self_attn7 = self_attention()\\n        self.self_attn8 = self_attention()\\n\\n\\n        # All the layers, we gonna need to make the decoder work.\\n        self.layer_norm = nn.LayerNorm(16)\\n        self.softmax = nn.Softmax(-1)\\n\\n        self.latent_downscale = nn.Linear(16, 2)\\n        self.latent_upscale = nn.Linear(2, 16)\\n\\n        self.final_linear_layer = nn.Linear(16, vocab_size) # out_features can be replaced with embedding dimension (at least, here).\\n\\n\\n    def forward(self, Q, K, V, X):\\n\\n        q = self.latent_downscale(Q)\\n        k = self.latent_downscale(K)\\n        v = self.latent_downscale(V)\\n\\n        x = self.latent_downscale(X)\\n        \\n        # getting the contexts from the respective self attention layers in the multi-headed attention block.\\n        context1 = self.self_attn1(q, k, v)\\n        context2 = self.self_attn2(q, k, v)\\n        context3 = self.self_attn3(q, k, v)\\n        context4 = self.self_attn4(q, k, v)\\n\\n        context5 = self.self_attn5(q, k, v)\\n        context6 = self.self_attn6(q, k, v)\\n        context7 = self.self_attn7(q, k, v)\\n        context8 = self.self_attn8(q, k, v)\\n\\n        # adding them up\\n        final_encodings = self.latent_upscale(context1 + context2 + context3 + context5 + context6 + context7 + context8 + x)\\n        final_encodings = self.layer_norm(final_encodings)\\n        logits = self.final_linear_layer(final_encodings)\\n\\n        return logits\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # creating the multi-headed attention block.\n",
    "        self.self_attn1 = self_attention()\n",
    "        self.self_attn2 = self_attention()\n",
    "        self.self_attn3 = self_attention()\n",
    "        self.self_attn4 = self_attention()\n",
    "\n",
    "        self.self_attn5 = self_attention()\n",
    "        self.self_attn6 = self_attention()\n",
    "        self.self_attn7 = self_attention()\n",
    "        self.self_attn8 = self_attention()\n",
    "\n",
    "\n",
    "        # All the layers, we gonna need to make the decoder work.\n",
    "        self.layer_norm = nn.LayerNorm(16)\n",
    "        self.softmax = nn.Softmax(-1)\n",
    "\n",
    "        self.latent_downscale = nn.Linear(16, 2)\n",
    "        self.latent_upscale = nn.Linear(2, 16)\n",
    "\n",
    "        self.final_linear_layer = nn.Linear(16, vocab_size) # out_features can be replaced with embedding dimension (at least, here).\n",
    "\n",
    "\n",
    "    def forward(self, Q, K, V, X):\n",
    "\n",
    "        q = self.latent_downscale(Q)\n",
    "        k = self.latent_downscale(K)\n",
    "        v = self.latent_downscale(V)\n",
    "\n",
    "        x = self.latent_downscale(X)\n",
    "        \n",
    "        # getting the contexts from the respective self attention layers in the multi-headed attention block.\n",
    "        context1 = self.self_attn1(q, k, v)\n",
    "        context2 = self.self_attn2(q, k, v)\n",
    "        context3 = self.self_attn3(q, k, v)\n",
    "        context4 = self.self_attn4(q, k, v)\n",
    "\n",
    "        context5 = self.self_attn5(q, k, v)\n",
    "        context6 = self.self_attn6(q, k, v)\n",
    "        context7 = self.self_attn7(q, k, v)\n",
    "        context8 = self.self_attn8(q, k, v)\n",
    "\n",
    "        # adding them up\n",
    "        final_encodings = self.latent_upscale(context1 + context2 + context3 + context5 + context6 + context7 + context8 + x)\n",
    "        final_encodings = self.layer_norm(final_encodings)\n",
    "        logits = self.final_linear_layer(final_encodings)\n",
    "\n",
    "        return logits\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # creating the multi-headed attention block.\n",
    "        self.self_attn1 = self_attention()\n",
    "        self.self_attn2 = self_attention()\n",
    "        self.self_attn3 = self_attention()\n",
    "        self.self_attn4 = self_attention()\n",
    "        self.self_attn5 = self_attention()\n",
    "        self.self_attn6 = self_attention()\n",
    "        self.self_attn7 = self_attention()\n",
    "        self.self_attn8 = self_attention()\n",
    "\n",
    "        self.self_attn9 = self_attention()\n",
    "        self.self_attn10 = self_attention()\n",
    "        self.self_attn11 = self_attention()\n",
    "        self.self_attn12 = self_attention()\n",
    "        self.self_attn13 = self_attention()\n",
    "        self.self_attn14 = self_attention()\n",
    "        self.self_attn15 = self_attention()\n",
    "        self.self_attn16 = self_attention()\n",
    "\n",
    "\n",
    "        # All the layers, we gonna need to make the decoder work.\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.softmax = nn.Softmax(-1)\n",
    "        \n",
    "        self.latent_downscale = nn.Linear(embedding_dim, 32)\n",
    "        self.latent_upscale = nn.Linear(32, embedding_dim)\n",
    "\n",
    "        self.final_linear_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, Q, K, V, X, attn_mask=None):\n",
    "        q = self.latent_downscale(Q)\n",
    "        k = self.latent_downscale(K)\n",
    "        v = self.latent_downscale(V)\n",
    "        x = self.latent_downscale(X)\n",
    "\n",
    "        context1 = self.self_attn1(q, k, v, attn_mask)\n",
    "        context2 = self.self_attn2(q, k, v, attn_mask)\n",
    "        context3 = self.self_attn3(q, k, v, attn_mask)\n",
    "        context4 = self.self_attn4(q, k, v, attn_mask)\n",
    "        context5 = self.self_attn5(q, k, v, attn_mask)\n",
    "        context6 = self.self_attn6(q, k, v, attn_mask)\n",
    "        context7 = self.self_attn7(q, k, v, attn_mask)\n",
    "        context8 = self.self_attn8(q, k, v, attn_mask)\n",
    "\n",
    "        context9 = self.self_attn1(q, k, v, attn_mask)\n",
    "        context10 = self.self_attn2(q, k, v, attn_mask)\n",
    "        context11 = self.self_attn3(q, k, v, attn_mask)\n",
    "        context12 = self.self_attn4(q, k, v, attn_mask)\n",
    "        context13 = self.self_attn5(q, k, v, attn_mask)\n",
    "        context14 = self.self_attn6(q, k, v, attn_mask)\n",
    "        context15 = self.self_attn7(q, k, v, attn_mask)\n",
    "        context16 = self.self_attn8(q, k, v, attn_mask)\n",
    "\n",
    "        combined = torch.cat((context1, context2, context3, context4, context5, context6, context7, context8, context9, context10, context11, context12, context13, context14, context15, context16), 2)\n",
    "        final_encodings = combined + self.latent_upscale(x)\n",
    "        final_encodings = self.layer_norm(final_encodings)\n",
    "        #logits = self.final_linear_layer(final_encodings)\n",
    "\n",
    "        return final_encodings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\\n\\nembedding_layer = nn.Embedding(vocab_size, embedding_dim).to(device)\\nmodel = Model().to(device)\\nPAD_IDX = vocab_to_index.get(\"<pad>\", 0)  # Ensure this is consistent with your vocab\\nloss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\\n\\n# Assuming: tokenized_lines = open(\"input.txt\").readlines(), vocab_to_idx built\\ndataset = ShakespeareDataset(tokenized_lines, vocab_to_index)\\nloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_batch)\\n\\ndef create_padding_mask(input_ids, pad_idx):\\n    input_ids: (batch, seq_len)\\n    return (input_ids != pad_idx).unsqueeze(1)  # (batch, 1, seq_len)\\n\\nfor epoch in range(10):\\n    total_loss = 0\\n\\n    for inputs, targets in loader:\\n        inputs, targets = inputs.to(device), targets.to(device)\\n        seq_len = inputs.size(1)\\n\\n        # Embeddings + positions\\n        embed = embedding_layer(inputs)  # (batch, seq_len, emb_dim)\\n        pos = positional_encodings(seq_len, embedding_dim, device)\\n        x = embed + pos\\n\\n        # Decoder input\\n        q = k = v = x\\n\\n        attn_mask = create_padding_mask(inputs, PAD_IDX).to(device)  # (batch, 1, seq_len)\\n        logits = model(q, k, v, x, attn_mask=attn_mask)\\n\\n        #print(\"inputs.dtype =\", inputs.dtype)\\n        logits = logits.view(-1, vocab_size)\\n        targets = targets.view(-1).long()  # Ensure targets are Long (int64)\\n\\n        # Ensure logits are float (if any issue with dtype mismatch)\\n        #logits = logits.long()\\n\\n        loss = loss_fn(logits, targets)\\n\\n        optimizer.zero_grad()\\n        loss.backward()\\n        optimizer.step()\\n\\n        total_loss += loss.item()\\n\\n    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "embedding_layer = nn.Embedding(vocab_size, embedding_dim).to(device)\n",
    "model = Model().to(device)\n",
    "PAD_IDX = vocab_to_index.get(\"<pad>\", 0)  # Ensure this is consistent with your vocab\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Assuming: tokenized_lines = open(\"input.txt\").readlines(), vocab_to_idx built\n",
    "dataset = ShakespeareDataset(tokenized_lines, vocab_to_index)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "def create_padding_mask(input_ids, pad_idx):\n",
    "    input_ids: (batch, seq_len)\n",
    "    return (input_ids != pad_idx).unsqueeze(1)  # (batch, 1, seq_len)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "\n",
    "    for inputs, targets in loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        seq_len = inputs.size(1)\n",
    "\n",
    "        # Embeddings + positions\n",
    "        embed = embedding_layer(inputs)  # (batch, seq_len, emb_dim)\n",
    "        pos = positional_encodings(seq_len, embedding_dim, device)\n",
    "        x = embed + pos\n",
    "\n",
    "        # Decoder input\n",
    "        q = k = v = x\n",
    "\n",
    "        attn_mask = create_padding_mask(inputs, PAD_IDX).to(device)  # (batch, 1, seq_len)\n",
    "        logits = model(q, k, v, x, attn_mask=attn_mask)\n",
    "\n",
    "        #print(\"inputs.dtype =\", inputs.dtype)\n",
    "        logits = logits.view(-1, vocab_size)\n",
    "        targets = targets.view(-1).long()  # Ensure targets are Long (int64)\n",
    "\n",
    "        # Ensure logits are float (if any issue with dtype mismatch)\n",
    "        #logits = logits.long()\n",
    "\n",
    "        loss = loss_fn(logits, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'store_res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m input_with_pos \u001b[38;5;241m=\u001b[39m input_with_pos\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     31\u001b[0m final_encodings \u001b[38;5;241m=\u001b[39m model(input_with_pos, input_with_pos, input_with_pos, input_with_pos)\n\u001b[0;32m---> 32\u001b[0m final_encodings \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(final_encodings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[43mstore_res\u001b[49m)\u001b[38;5;241m.\u001b[39mto(device)(final_encodings\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     33\u001b[0m logits \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(embedding_dim, vocab_size)\u001b[38;5;241m.\u001b[39mto(device)(final_encodings\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     35\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size), targets\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'store_res' is not defined"
     ]
    }
   ],
   "source": [
    "embedding_layer = nn.Embedding(vocab_size, embedding_dim).to(device)\n",
    "#model = Model().to(device)\n",
    "PAD_IDX = vocab_to_index.get(\"<pad>\", 0)  # Ensure this is consistent with your vocab\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Assuming: tokenized_lines = open(\"input.txt\").readlines(), vocab_to_idx built\n",
    "dataset = ShakespeareDataset(tokenized_lines, vocab_to_index)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "def create_padding_mask(input_ids, pad_idx):\n",
    "    input_ids: (batch, seq_len)\n",
    "    return (input_ids != pad_idx).unsqueeze(1)  # (batch, 1, seq_len)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "\n",
    "    for inputs, targets in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Get embeddings\n",
    "        input_embeddings = embedding_layer(inputs)   \n",
    "        pos_enc = positional_encodings(input_embeddings.size(1), embedding_dim, device)\n",
    "        input_with_pos = input_embeddings + pos_enc\n",
    "\n",
    "        store_res = input_with_pos.shape[1]\n",
    "\n",
    "        input_with_pos = nn.Linear(input_with_pos.shape[1], 5).to(device)(input_with_pos.transpose(-2, -1))\n",
    "        input_with_pos = input_with_pos.transpose(-2, -1)\n",
    "\n",
    "        final_encodings = model(input_with_pos, input_with_pos, input_with_pos, input_with_pos)\n",
    "        final_encodings = nn.Linear(final_encodings.shape[1], store_res).to(device)(final_encodings.transpose(-2, -1))\n",
    "        logits = nn.Linear(embedding_dim, vocab_size).to(device)(final_encodings.transpose(-2, -1))\n",
    "        \n",
    "        loss = loss_fn(logits.view(-1, vocab_size), targets.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accuracy\n",
    "        predicted = torch.argmax(logits, dim=-1)\n",
    "        correct = (predicted == targets).float()\n",
    "        mask = (targets != PAD_IDX).float()\n",
    "        accuracy = (correct * mask).sum() / mask.sum()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_accuracy += accuracy.item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    avg_accuracy = total_accuracy / len(loader)\n",
    "    #print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.4f}\")\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss}, Accuracy: {avg_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.00095293298364"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([20827, 12845,  9796, 13784,  1096,     0,     0,     0,     0,     0])\n",
      "Target: tensor([12845,  9796, 13784,  1096, 22793,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "for x, y in loader:\n",
    "    print(\"Input:\", x[0])\n",
    "    print(\"Target:\", y[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "<start> tale sir, a careful height will be absent. wend of thy name. charge. caps thing; eye, cause procures with old tale, help. times and yet most piteous woes hung long, who's here! provost, thinkest glory. and that name became is't possible friend of sorrow wind betwixt as i said, dearly\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def generate_sequence(model, start_text, vocab_to_idx, idx_to_vocab, embedding_layer, device, max_len=50):\n",
    "    model.eval()  # Evaluation mode\n",
    "    start_tokens = start_text.lower().split()\n",
    "\n",
    "    # Convert words to indices\n",
    "    input_ids = [vocab_to_idx.get(word, vocab_to_idx[\"<pad>\"]) for word in start_tokens]\n",
    "    generated = torch.tensor(input_ids, dtype=torch.long, device=device).unsqueeze(0)  # (1, seq_len)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        seq_len = generated.size(1)\n",
    "\n",
    "        # Recalculate positional encodings each time\n",
    "        pos = positional_encodings(seq_len, embedding_layer.embedding_dim, device)\n",
    "        input_embed = embedding_layer(generated) + pos\n",
    "\n",
    "        # Attention mask\n",
    "        attn_mask = create_padding_mask(generated, vocab_to_idx[\"<pad>\"]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q = k = v = input_embed\n",
    "            logits = model(q, k, v, input_embed, attn_mask)\n",
    "\n",
    "        # Sample next token\n",
    "        logits = logits[:, -1, :]  # Get last token's logits\n",
    "        temperature = 0.7\n",
    "        probs = torch.softmax(logits / temperature, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)  # Shape: (1, 1)\n",
    "\n",
    "        # Stop if end token\n",
    "        token_id = next_token.item()\n",
    "        if idx_to_vocab.get(token_id, \"\") == \"<end>\":\n",
    "            break\n",
    "\n",
    "        # Append next token\n",
    "        generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "    # Convert generated indices back to words\n",
    "    generated_text = ' '.join([idx_to_vocab.get(idx.item(), \"<unk>\") for idx in generated.squeeze()])\n",
    "    return generated_text\n",
    "\n",
    "# Example of inference usage:\n",
    "start_text = \"<start>\"  # Starting text for generation\n",
    "generated_text = generate_sequence(\n",
    "    model=model, \n",
    "    start_text=start_text, \n",
    "    vocab_to_idx=vocab_to_index, \n",
    "    idx_to_vocab={index: word for word, index in vocab_to_index.items()}, \n",
    "    embedding_layer=embedding_layer, \n",
    "    device=device,\n",
    "    max_len=50  # Limit generated sequence length\n",
    ")\n",
    "\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
