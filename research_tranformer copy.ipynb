{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_lines = open(\"input.txt\", \"r\")\n",
    "tokenized_lines = tokenized_lines.readlines()\n",
    "\n",
    "vocab = set()\n",
    "special_tokens = [\"<pad>\", \"<start>\", \"<end>\"]\n",
    "for sentence in tokenized_lines:\n",
    "    vocab.update(sentence.split())\n",
    "vocab = special_tokens + list(vocab)\n",
    "\n",
    "vocab_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "vocab_size = len(vocab)\n",
    "#print(vocab)\n",
    "#print(\"Vocab size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "PAD_IDX = vocab_to_index[PAD_TOKEN]\n",
    "\n",
    "def collate_batch(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "\n",
    "    #inputs = [torch.tensor(seq, dtype = torch.long()) for seq in inputs]\n",
    "    #targets = [torch.tensor(seq, dtype = torch.long()) for seq in targets]\n",
    "\n",
    "    padded_inputs = pad_sequence(inputs, batch_first=True, padding_value=PAD_IDX)\n",
    "    padded_targets = pad_sequence(targets, batch_first=True, padding_value=PAD_IDX)\n",
    "\n",
    "    return padded_inputs, padded_targets\n",
    "\n",
    "\n",
    "# 1. Rebuild vocab from lowercased text and include <unk>\n",
    "special_tokens = [\"<pad>\", \"<start>\", \"<end>\", \"<unk>\"]\n",
    "\n",
    "vocab_to_index = {}\n",
    "\n",
    "vocab = set()\n",
    "for sentence in tokenized_lines:\n",
    "    vocab.update(sentence.lower().split())      # lowercase here\n",
    "\n",
    "vocab = special_tokens + sorted(vocab)          # sorted for reproducibility\n",
    "vocab_to_index = {w:i for i,w in enumerate(vocab)}\n",
    "\n",
    "PAD_IDX = vocab_to_index[\"<pad>\"]\n",
    "UNK_IDX = vocab_to_index[\"<unk>\"]\n",
    "\n",
    "# 2. Update your Dataset to use .get(â€¦, UNK_IDX) instead of direct indexing\n",
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, tokenized_lines, vocab_to_idx):\n",
    "        self.data = [\n",
    "            line.lower().split()\n",
    "            for line in tokenized_lines\n",
    "            if len(line.lower().split()) > 2  # ignore short lines\n",
    "        ]\n",
    "        self.vocab_to_idx = vocab_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        words = self.data[idx]\n",
    "\n",
    "        # THIS should raise error if token is missing\n",
    "        input_ids = [self.vocab_to_idx.get(word, self.vocab_to_idx[\"<unk>\"]) for word in words[:-1]]\n",
    "        target_ids = [self.vocab_to_idx.get(word, self.vocab_to_idx[\"<unk>\"]) for word in words[1:]]\n",
    "\n",
    "        return torch.tensor(input_ids, dtype=torch.long), torch.tensor(target_ids, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encodings(seq_len, embedding_dim, device):\n",
    "    position = torch.arange(seq_len, dtype=torch.float, device=device).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, embedding_dim, 2, device=device).float() * (-math.log(10000.0) / embedding_dim))\n",
    "    pe = torch.zeros(seq_len, embedding_dim, device=device)\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(self_attention, self).__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask=None):\n",
    "        # Q, K, V shape: (batch, seq_len, dim)\n",
    "        batch_size, seq_len, dim = Q.size()\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(dim)  # (batch, seq_len, seq_len)\n",
    "\n",
    "        # Causal mask (upper triangular)\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=Q.device) * float('-inf'), diagonal=1)\n",
    "        scores = scores + causal_mask\n",
    "\n",
    "        # Padding mask (optional)\n",
    "        if attn_mask is not None:\n",
    "            # attn_mask: (batch, 1, seq_len), 1 for keep, 0 for mask\n",
    "            scores = scores.masked_fill(attn_mask == 0, float('-inf'))\n",
    "\n",
    "        weights = self.softmax(scores)\n",
    "        context = torch.matmul(weights, V)  # (batch, seq_len, dim)\n",
    "\n",
    "        return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # creating the multi-headed attention block.\n",
    "        self.self_attn1 = self_attention()\n",
    "        self.self_attn2 = self_attention()\n",
    "        self.self_attn3 = self_attention()\n",
    "        self.self_attn4 = self_attention()\n",
    "        self.self_attn5 = self_attention()\n",
    "        self.self_attn6 = self_attention()\n",
    "        self.self_attn7 = self_attention()\n",
    "        self.self_attn8 = self_attention()\n",
    "\n",
    "        self.self_attn9 = self_attention()\n",
    "        self.self_attn10 = self_attention()\n",
    "        self.self_attn11 = self_attention()\n",
    "        self.self_attn12 = self_attention()\n",
    "        self.self_attn13 = self_attention()\n",
    "        self.self_attn14 = self_attention()\n",
    "        self.self_attn15 = self_attention()\n",
    "        self.self_attn16 = self_attention()\n",
    "\n",
    "\n",
    "        # All the layers, we gonna need to make the decoder work.\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.softmax = nn.Softmax(-1)\n",
    "        \n",
    "        self.latent_downscale = nn.Linear(embedding_dim, 32)\n",
    "        self.latent_upscale = nn.Linear(32, embedding_dim)\n",
    "\n",
    "        self.final_linear_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, Q, K, V, X, attn_mask=None):\n",
    "        q = self.latent_downscale(Q)\n",
    "        k = self.latent_downscale(K)\n",
    "        v = self.latent_downscale(V)\n",
    "        x = self.latent_downscale(X)\n",
    "\n",
    "        context1 = self.self_attn1(q, k, v, attn_mask)\n",
    "        context2 = self.self_attn2(q, k, v, attn_mask)\n",
    "        context3 = self.self_attn3(q, k, v, attn_mask)\n",
    "        context4 = self.self_attn4(q, k, v, attn_mask)\n",
    "        context5 = self.self_attn5(q, k, v, attn_mask)\n",
    "        context6 = self.self_attn6(q, k, v, attn_mask)\n",
    "        context7 = self.self_attn7(q, k, v, attn_mask)\n",
    "        context8 = self.self_attn8(q, k, v, attn_mask)\n",
    "\n",
    "        context9 = self.self_attn1(q, k, v, attn_mask)\n",
    "        context10 = self.self_attn2(q, k, v, attn_mask)\n",
    "        context11 = self.self_attn3(q, k, v, attn_mask)\n",
    "        context12 = self.self_attn4(q, k, v, attn_mask)\n",
    "        context13 = self.self_attn5(q, k, v, attn_mask)\n",
    "        context14 = self.self_attn6(q, k, v, attn_mask)\n",
    "        context15 = self.self_attn7(q, k, v, attn_mask)\n",
    "        context16 = self.self_attn8(q, k, v, attn_mask)\n",
    "\n",
    "        combined = torch.cat((context1, context2, context3, context4, context5, context6, context7, context8, context9, context10, context11, context12, context13, context14, context15, context16), 2)\n",
    "        final_encodings = combined + self.latent_upscale(x)\n",
    "        final_encodings = self.layer_norm(final_encodings)\n",
    "        #logits = self.final_linear_layer(final_encodings)\n",
    "\n",
    "        return final_encodings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 10.209327824115753, Accuracy: 2.404593804385513e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size), targets\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     38\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 39\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Accuracy\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embedding_layer = nn.Embedding(vocab_size, embedding_dim).to(device)\n",
    "#model = Model().to(device)\n",
    "PAD_IDX = vocab_to_index.get(\"<pad>\", 0)  # Ensure this is consistent with your vocab\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Assuming: tokenized_lines = open(\"input.txt\").readlines(), vocab_to_idx built\n",
    "dataset = ShakespeareDataset(tokenized_lines, vocab_to_index)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "def create_padding_mask(input_ids, pad_idx):\n",
    "    input_ids: (batch, seq_len)\n",
    "    return (input_ids != pad_idx).unsqueeze(1)  # (batch, 1, seq_len)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "\n",
    "    for inputs, targets in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Get embeddings\n",
    "        input_embeddings = embedding_layer(inputs)   \n",
    "        pos_enc = positional_encodings(input_embeddings.size(1), embedding_dim, device)\n",
    "        input_with_pos = input_embeddings + pos_enc\n",
    "\n",
    "        store_res = input_with_pos.shape[1]\n",
    "\n",
    "        input_with_pos = nn.Linear(input_with_pos.shape[1], 5).to(device)(input_with_pos.transpose(-2, -1))\n",
    "        input_with_pos = input_with_pos.transpose(-2, -1)\n",
    "\n",
    "        final_encodings = model(input_with_pos, input_with_pos, input_with_pos, input_with_pos)\n",
    "        final_encodings = nn.Linear(final_encodings.shape[1], store_res).to(device)(final_encodings.transpose(-2, -1))\n",
    "        logits = nn.Linear(embedding_dim, vocab_size).to(device)(final_encodings.transpose(-2, -1))\n",
    "        \n",
    "        loss = loss_fn(logits.view(-1, vocab_size), targets.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accuracy\n",
    "        predicted = torch.argmax(logits, dim=-1)\n",
    "        correct = (predicted == targets).float()\n",
    "        mask = (targets != PAD_IDX).float()\n",
    "        accuracy = (correct * mask).sum() / mask.sum()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_accuracy += accuracy.item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    avg_accuracy = total_accuracy / len(loader)\n",
    "    #print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.4f}\")\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss}, Accuracy: {avg_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.00095293298364"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([20827, 12845,  9796, 13784,  1096,     0,     0,     0,     0,     0])\n",
      "Target: tensor([12845,  9796, 13784,  1096, 22793,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "for x, y in loader:\n",
    "    print(\"Input:\", x[0])\n",
    "    print(\"Target:\", y[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "<start> tale sir, a careful height will be absent. wend of thy name. charge. caps thing; eye, cause procures with old tale, help. times and yet most piteous woes hung long, who's here! provost, thinkest glory. and that name became is't possible friend of sorrow wind betwixt as i said, dearly\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def generate_sequence(model, start_text, vocab_to_idx, idx_to_vocab, embedding_layer, device, max_len=50):\n",
    "    model.eval()  # Evaluation mode\n",
    "    start_tokens = start_text.lower().split()\n",
    "\n",
    "    # Convert words to indices\n",
    "    input_ids = [vocab_to_idx.get(word, vocab_to_idx[\"<pad>\"]) for word in start_tokens]\n",
    "    generated = torch.tensor(input_ids, dtype=torch.long, device=device).unsqueeze(0)  # (1, seq_len)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        seq_len = generated.size(1)\n",
    "\n",
    "        # Recalculate positional encodings each time\n",
    "        pos = positional_encodings(seq_len, embedding_layer.embedding_dim, device)\n",
    "        input_embed = embedding_layer(generated) + pos\n",
    "\n",
    "        # Attention mask\n",
    "        attn_mask = create_padding_mask(generated, vocab_to_idx[\"<pad>\"]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q = k = v = input_embed\n",
    "            logits = model(q, k, v, input_embed, attn_mask)\n",
    "\n",
    "        # Sample next token\n",
    "        logits = logits[:, -1, :]  # Get last token's logits\n",
    "        temperature = 0.7\n",
    "        probs = torch.softmax(logits / temperature, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)  # Shape: (1, 1)\n",
    "\n",
    "        # Stop if end token\n",
    "        token_id = next_token.item()\n",
    "        if idx_to_vocab.get(token_id, \"\") == \"<end>\":\n",
    "            break\n",
    "\n",
    "        # Append next token\n",
    "        generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "    # Convert generated indices back to words\n",
    "    generated_text = ' '.join([idx_to_vocab.get(idx.item(), \"<unk>\") for idx in generated.squeeze()])\n",
    "    return generated_text\n",
    "\n",
    "# Example of inference usage:\n",
    "start_text = \"<start>\"  # Starting text for generation\n",
    "generated_text = generate_sequence(\n",
    "    model=model, \n",
    "    start_text=start_text, \n",
    "    vocab_to_idx=vocab_to_index, \n",
    "    idx_to_vocab={index: word for word, index in vocab_to_index.items()}, \n",
    "    embedding_layer=embedding_layer, \n",
    "    device=device,\n",
    "    max_len=50  # Limit generated sequence length\n",
    ")\n",
    "\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
