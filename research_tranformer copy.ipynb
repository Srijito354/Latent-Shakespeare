{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_lines = open(\"input.txt\", \"r\")\n",
    "tokenized_lines = tokenized_lines.readlines()\n",
    "\n",
    "vocab = set()\n",
    "special_tokens = [\"<pad>\", \"<start>\", \"<end>\"]\n",
    "for sentence in tokenized_lines:\n",
    "    vocab.update(sentence.split())\n",
    "vocab = special_tokens + list(vocab)\n",
    "\n",
    "vocab_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "vocab_size = len(vocab)\n",
    "#print(vocab)\n",
    "#print(\"Vocab size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "PAD_IDX = vocab_to_index[PAD_TOKEN]\n",
    "\n",
    "def collate_batch(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "\n",
    "    #inputs = [torch.tensor(seq, dtype = torch.long()) for seq in inputs]\n",
    "    #targets = [torch.tensor(seq, dtype = torch.long()) for seq in targets]\n",
    "\n",
    "    padded_inputs = pad_sequence(inputs, batch_first=True, padding_value=PAD_IDX)\n",
    "    padded_targets = pad_sequence(targets, batch_first=True, padding_value=PAD_IDX)\n",
    "\n",
    "    return padded_inputs, padded_targets\n",
    "\n",
    "\n",
    "# 1. Rebuild vocab from lowercased text and include <unk>\n",
    "special_tokens = [\"<pad>\", \"<start>\", \"<end>\", \"<unk>\"]\n",
    "\n",
    "vocab_to_index = {}\n",
    "\n",
    "vocab = set()\n",
    "for sentence in tokenized_lines:\n",
    "    vocab.update(sentence.lower().split())      # lowercase here\n",
    "\n",
    "vocab = special_tokens + sorted(vocab)          # sorted for reproducibility\n",
    "vocab_to_index = {w:i for i,w in enumerate(vocab)}\n",
    "\n",
    "PAD_IDX = vocab_to_index[\"<pad>\"]\n",
    "UNK_IDX = vocab_to_index[\"<unk>\"]\n",
    "\n",
    "# 2. Update your Dataset to use .get(â€¦, UNK_IDX) instead of direct indexing\n",
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, tokenized_lines, vocab_to_idx):\n",
    "        self.data = [\n",
    "            line.lower().split()\n",
    "            for line in tokenized_lines\n",
    "            if len(line.lower().split()) > 2  # ignore short lines\n",
    "        ]\n",
    "        self.vocab_to_idx = vocab_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        words = self.data[idx]\n",
    "\n",
    "        # THIS should raise error if token is missing\n",
    "        input_ids = [self.vocab_to_idx.get(word, self.vocab_to_idx[\"<unk>\"]) for word in words[:-1]]\n",
    "        target_ids = [self.vocab_to_idx.get(word, self.vocab_to_idx[\"<unk>\"]) for word in words[1:]]\n",
    "\n",
    "        return torch.tensor(input_ids, dtype=torch.long), torch.tensor(target_ids, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndata = [\\n    line.lower().split()\\n    for line in tokenized_lines\\n    if len(line.lower().split()) > 2  # ignore short lines\\n]\\nfor i in range(5):\\n    words = data[i]\\n    print(words[:-1])\\n    print(words[1:])\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "data = [\n",
    "    line.lower().split()\n",
    "    for line in tokenized_lines\n",
    "    if len(line.lower().split()) > 2  # ignore short lines\n",
    "]\n",
    "for i in range(5):\n",
    "    words = data[i]\n",
    "    print(words[:-1])\n",
    "    print(words[1:])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encodings(seq_len, embedding_dim, device):\n",
    "    position = torch.arange(seq_len, dtype=torch.float, device=device).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, embedding_dim, 2, device=device).float() * (-math.log(10000.0) / embedding_dim))\n",
    "    pe = torch.zeros(seq_len, embedding_dim, device=device)\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nclass self_attention(nn.Module):\\n    def __init__(self):\\n        super(self_attention, self).__init__()\\n        #self.w_qkv_downscale = nn.Linear(in_channels=16, out_channels=2)\\n        #self.latent_upscale = nn.Linear(in_channels=2, out_channels=16)\\n        #self.layer_norm = nn.LayerNorm()\\n        self.softmax = nn.Softmax()\\n\\n    def forward(self, Q, K, V):\\n        #Q = self.w_qkv_downscale(Q)\\n        #K = self.w_qkv_downscale(K)\\n        #V = self.w_qkv_downscale(V)\\n        seq_len = Q.size(1)\\n        mask = torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1)\\n        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (16 ** 0.5) + mask\\n        #attention_weights = self.softmax(attention_scores, dim=-1)\\n        attention_weights = self.softmax(attention_scores)\\n        context = torch.matmul(attention_weights, V)\\n\\n        #context = self.latent_upscale(context)\\n\\n        # Residual + Norm\\n        # x = self.layer_norm(context + x)\\n\\n        # Feedforward + Norm\\n        #ff_out = self.feed_fwd(x)\\n        #out = self.layer_norm(ff_out + x)\\n\\n        # Final linear (optional)\\n        #return self.output_proj(out)\\n        return context\\n\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "class self_attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(self_attention, self).__init__()\n",
    "        #self.w_qkv_downscale = nn.Linear(in_channels=16, out_channels=2)\n",
    "        #self.latent_upscale = nn.Linear(in_channels=2, out_channels=16)\n",
    "        #self.layer_norm = nn.LayerNorm()\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        #Q = self.w_qkv_downscale(Q)\n",
    "        #K = self.w_qkv_downscale(K)\n",
    "        #V = self.w_qkv_downscale(V)\n",
    "        seq_len = Q.size(1)\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1)\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (16 ** 0.5) + mask\n",
    "        #attention_weights = self.softmax(attention_scores, dim=-1)\n",
    "        attention_weights = self.softmax(attention_scores)\n",
    "        context = torch.matmul(attention_weights, V)\n",
    "\n",
    "        #context = self.latent_upscale(context)\n",
    "\n",
    "        # Residual + Norm\n",
    "        # x = self.layer_norm(context + x)\n",
    "\n",
    "        # Feedforward + Norm\n",
    "        #ff_out = self.feed_fwd(x)\n",
    "        #out = self.layer_norm(ff_out + x)\n",
    "\n",
    "        # Final linear (optional)\n",
    "        #return self.output_proj(out)\n",
    "        return context\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(self_attention, self).__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask=None):\n",
    "        # Q, K, V shape: (batch, seq_len, dim)\n",
    "        batch_size, seq_len, dim = Q.size()\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(dim)  # (batch, seq_len, seq_len)\n",
    "\n",
    "        # Causal mask (upper triangular)\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=Q.device) * float('-inf'), diagonal=1)\n",
    "        scores = scores + causal_mask\n",
    "\n",
    "        # Padding mask (optional)\n",
    "        if attn_mask is not None:\n",
    "            # attn_mask: (batch, 1, seq_len), 1 for keep, 0 for mask\n",
    "            scores = scores.masked_fill(attn_mask == 0, float('-inf'))\n",
    "\n",
    "        weights = self.softmax(scores)\n",
    "        context = torch.matmul(weights, V)  # (batch, seq_len, dim)\n",
    "\n",
    "        return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass Model(nn.Module):\\n    def __init__(self):\\n        super(Model, self).__init__()\\n        \\n        # creating the multi-headed attention block.\\n        self.self_attn1 = self_attention()\\n        self.self_attn2 = self_attention()\\n        self.self_attn3 = self_attention()\\n        self.self_attn4 = self_attention()\\n\\n        self.self_attn5 = self_attention()\\n        self.self_attn6 = self_attention()\\n        self.self_attn7 = self_attention()\\n        self.self_attn8 = self_attention()\\n\\n\\n        # All the layers, we gonna need to make the decoder work.\\n        self.layer_norm = nn.LayerNorm(16)\\n        self.softmax = nn.Softmax(-1)\\n\\n        self.latent_downscale = nn.Linear(16, 2)\\n        self.latent_upscale = nn.Linear(2, 16)\\n\\n        self.final_linear_layer = nn.Linear(16, vocab_size) # out_features can be replaced with embedding dimension (at least, here).\\n\\n\\n    def forward(self, Q, K, V, X):\\n\\n        q = self.latent_downscale(Q)\\n        k = self.latent_downscale(K)\\n        v = self.latent_downscale(V)\\n\\n        x = self.latent_downscale(X)\\n        \\n        # getting the contexts from the respective self attention layers in the multi-headed attention block.\\n        context1 = self.self_attn1(q, k, v)\\n        context2 = self.self_attn2(q, k, v)\\n        context3 = self.self_attn3(q, k, v)\\n        context4 = self.self_attn4(q, k, v)\\n\\n        context5 = self.self_attn5(q, k, v)\\n        context6 = self.self_attn6(q, k, v)\\n        context7 = self.self_attn7(q, k, v)\\n        context8 = self.self_attn8(q, k, v)\\n\\n        # adding them up\\n        final_encodings = self.latent_upscale(context1 + context2 + context3 + context5 + context6 + context7 + context8 + x)\\n        final_encodings = self.layer_norm(final_encodings)\\n        logits = self.final_linear_layer(final_encodings)\\n\\n        return logits\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # creating the multi-headed attention block.\n",
    "        self.self_attn1 = self_attention()\n",
    "        self.self_attn2 = self_attention()\n",
    "        self.self_attn3 = self_attention()\n",
    "        self.self_attn4 = self_attention()\n",
    "\n",
    "        self.self_attn5 = self_attention()\n",
    "        self.self_attn6 = self_attention()\n",
    "        self.self_attn7 = self_attention()\n",
    "        self.self_attn8 = self_attention()\n",
    "\n",
    "\n",
    "        # All the layers, we gonna need to make the decoder work.\n",
    "        self.layer_norm = nn.LayerNorm(16)\n",
    "        self.softmax = nn.Softmax(-1)\n",
    "\n",
    "        self.latent_downscale = nn.Linear(16, 2)\n",
    "        self.latent_upscale = nn.Linear(2, 16)\n",
    "\n",
    "        self.final_linear_layer = nn.Linear(16, vocab_size) # out_features can be replaced with embedding dimension (at least, here).\n",
    "\n",
    "\n",
    "    def forward(self, Q, K, V, X):\n",
    "\n",
    "        q = self.latent_downscale(Q)\n",
    "        k = self.latent_downscale(K)\n",
    "        v = self.latent_downscale(V)\n",
    "\n",
    "        x = self.latent_downscale(X)\n",
    "        \n",
    "        # getting the contexts from the respective self attention layers in the multi-headed attention block.\n",
    "        context1 = self.self_attn1(q, k, v)\n",
    "        context2 = self.self_attn2(q, k, v)\n",
    "        context3 = self.self_attn3(q, k, v)\n",
    "        context4 = self.self_attn4(q, k, v)\n",
    "\n",
    "        context5 = self.self_attn5(q, k, v)\n",
    "        context6 = self.self_attn6(q, k, v)\n",
    "        context7 = self.self_attn7(q, k, v)\n",
    "        context8 = self.self_attn8(q, k, v)\n",
    "\n",
    "        # adding them up\n",
    "        final_encodings = self.latent_upscale(context1 + context2 + context3 + context5 + context6 + context7 + context8 + x)\n",
    "        final_encodings = self.layer_norm(final_encodings)\n",
    "        logits = self.final_linear_layer(final_encodings)\n",
    "\n",
    "        return logits\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # creating the multi-headed attention block.\n",
    "        self.self_attn1 = self_attention()\n",
    "        self.self_attn2 = self_attention()\n",
    "        self.self_attn3 = self_attention()\n",
    "        self.self_attn4 = self_attention()\n",
    "        self.self_attn5 = self_attention()\n",
    "        self.self_attn6 = self_attention()\n",
    "        self.self_attn7 = self_attention()\n",
    "        self.self_attn8 = self_attention()\n",
    "\n",
    "        self.self_attn9 = self_attention()\n",
    "        self.self_attn10 = self_attention()\n",
    "        self.self_attn11 = self_attention()\n",
    "        self.self_attn12 = self_attention()\n",
    "        self.self_attn13 = self_attention()\n",
    "        self.self_attn14 = self_attention()\n",
    "        self.self_attn15 = self_attention()\n",
    "        self.self_attn16 = self_attention()\n",
    "\n",
    "\n",
    "        # All the layers, we gonna need to make the decoder work.\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.softmax = nn.Softmax(-1)\n",
    "        \n",
    "        self.latent_downscale = nn.Linear(embedding_dim, 32)\n",
    "        self.latent_upscale = nn.Linear(32, embedding_dim)\n",
    "\n",
    "        self.final_linear_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, Q, K, V, X, attn_mask=None):\n",
    "        q = self.latent_downscale(Q)\n",
    "        k = self.latent_downscale(K)\n",
    "        v = self.latent_downscale(V)\n",
    "        x = self.latent_downscale(X)\n",
    "\n",
    "        context1 = self.self_attn1(q, k, v, attn_mask)\n",
    "        context2 = self.self_attn2(q, k, v, attn_mask)\n",
    "        context3 = self.self_attn3(q, k, v, attn_mask)\n",
    "        context4 = self.self_attn4(q, k, v, attn_mask)\n",
    "        context5 = self.self_attn5(q, k, v, attn_mask)\n",
    "        context6 = self.self_attn6(q, k, v, attn_mask)\n",
    "        context7 = self.self_attn7(q, k, v, attn_mask)\n",
    "        context8 = self.self_attn8(q, k, v, attn_mask)\n",
    "\n",
    "        context9 = self.self_attn1(q, k, v, attn_mask)\n",
    "        context10 = self.self_attn2(q, k, v, attn_mask)\n",
    "        context11 = self.self_attn3(q, k, v, attn_mask)\n",
    "        context12 = self.self_attn4(q, k, v, attn_mask)\n",
    "        context13 = self.self_attn5(q, k, v, attn_mask)\n",
    "        context14 = self.self_attn6(q, k, v, attn_mask)\n",
    "        context15 = self.self_attn7(q, k, v, attn_mask)\n",
    "        context16 = self.self_attn8(q, k, v, attn_mask)\n",
    "\n",
    "        combined = torch.cat((context1, context2, context3, context4, context5, context6, context7, context8, context9, context10, context11, context12, context13, context14, context15, context16), 2)\n",
    "        final_encodings = combined + self.latent_upscale(x)\n",
    "        final_encodings = self.layer_norm(final_encodings)\n",
    "        logits = self.final_linear_layer(final_encodings)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\\n\\nembedding_layer = nn.Embedding(vocab_size, embedding_dim).to(device)\\nmodel = Model().to(device)\\nPAD_IDX = vocab_to_index.get(\"<pad>\", 0)  # Ensure this is consistent with your vocab\\nloss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\\n\\n# Assuming: tokenized_lines = open(\"input.txt\").readlines(), vocab_to_idx built\\ndataset = ShakespeareDataset(tokenized_lines, vocab_to_index)\\nloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_batch)\\n\\ndef create_padding_mask(input_ids, pad_idx):\\n    input_ids: (batch, seq_len)\\n    return (input_ids != pad_idx).unsqueeze(1)  # (batch, 1, seq_len)\\n\\nfor epoch in range(10):\\n    total_loss = 0\\n\\n    for inputs, targets in loader:\\n        inputs, targets = inputs.to(device), targets.to(device)\\n        seq_len = inputs.size(1)\\n\\n        # Embeddings + positions\\n        embed = embedding_layer(inputs)  # (batch, seq_len, emb_dim)\\n        pos = positional_encodings(seq_len, embedding_dim, device)\\n        x = embed + pos\\n\\n        # Decoder input\\n        q = k = v = x\\n\\n        attn_mask = create_padding_mask(inputs, PAD_IDX).to(device)  # (batch, 1, seq_len)\\n        logits = model(q, k, v, x, attn_mask=attn_mask)\\n\\n        #print(\"inputs.dtype =\", inputs.dtype)\\n        logits = logits.view(-1, vocab_size)\\n        targets = targets.view(-1).long()  # Ensure targets are Long (int64)\\n\\n        # Ensure logits are float (if any issue with dtype mismatch)\\n        #logits = logits.long()\\n\\n        loss = loss_fn(logits, targets)\\n\\n        optimizer.zero_grad()\\n        loss.backward()\\n        optimizer.step()\\n\\n        total_loss += loss.item()\\n\\n    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "embedding_layer = nn.Embedding(vocab_size, embedding_dim).to(device)\n",
    "model = Model().to(device)\n",
    "PAD_IDX = vocab_to_index.get(\"<pad>\", 0)  # Ensure this is consistent with your vocab\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Assuming: tokenized_lines = open(\"input.txt\").readlines(), vocab_to_idx built\n",
    "dataset = ShakespeareDataset(tokenized_lines, vocab_to_index)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "def create_padding_mask(input_ids, pad_idx):\n",
    "    input_ids: (batch, seq_len)\n",
    "    return (input_ids != pad_idx).unsqueeze(1)  # (batch, 1, seq_len)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "\n",
    "    for inputs, targets in loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        seq_len = inputs.size(1)\n",
    "\n",
    "        # Embeddings + positions\n",
    "        embed = embedding_layer(inputs)  # (batch, seq_len, emb_dim)\n",
    "        pos = positional_encodings(seq_len, embedding_dim, device)\n",
    "        x = embed + pos\n",
    "\n",
    "        # Decoder input\n",
    "        q = k = v = x\n",
    "\n",
    "        attn_mask = create_padding_mask(inputs, PAD_IDX).to(device)  # (batch, 1, seq_len)\n",
    "        logits = model(q, k, v, x, attn_mask=attn_mask)\n",
    "\n",
    "        #print(\"inputs.dtype =\", inputs.dtype)\n",
    "        logits = logits.view(-1, vocab_size)\n",
    "        targets = targets.view(-1).long()  # Ensure targets are Long (int64)\n",
    "\n",
    "        # Ensure logits are float (if any issue with dtype mismatch)\n",
    "        #logits = logits.long()\n",
    "\n",
    "        loss = loss_fn(logits, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 7.477916904091835, Accuracy: 0.055202283542021176\n",
      "Epoch 2, Loss: 6.686478775143623, Accuracy: 0.072087163743563\n",
      "Epoch 3, Loss: 6.357288537025451, Accuracy: 0.07524879801087081\n",
      "Epoch 4, Loss: 6.153560679554939, Accuracy: 0.0763084243517369\n",
      "Epoch 5, Loss: 6.003016156554222, Accuracy: 0.07782476340187713\n",
      "Epoch 6, Loss: 5.888869621157646, Accuracy: 0.07946537273935973\n",
      "Epoch 7, Loss: 5.802793600559235, Accuracy: 0.08224688844755292\n",
      "Epoch 8, Loss: 5.731844701170921, Accuracy: 0.0850074894586578\n",
      "Epoch 9, Loss: 5.676612593531608, Accuracy: 0.0882720236782916\n",
      "Epoch 10, Loss: 5.630820688605309, Accuracy: 0.08936731716152281\n",
      "Epoch 11, Loss: 5.593530374765396, Accuracy: 0.09229876578785479\n",
      "Epoch 12, Loss: 5.561058477759361, Accuracy: 0.09356124300975352\n",
      "Epoch 13, Loss: 5.532365618944168, Accuracy: 0.09513378794072196\n",
      "Epoch 14, Loss: 5.504541159272194, Accuracy: 0.09609870825894178\n",
      "Epoch 15, Loss: 5.483619874119759, Accuracy: 0.09766070100478828\n",
      "Epoch 16, Loss: 5.462584271430969, Accuracy: 0.09858289453666658\n",
      "Epoch 17, Loss: 5.4429247391223905, Accuracy: 0.0989171826140955\n",
      "Epoch 18, Loss: 5.424343836903572, Accuracy: 0.10086128677707165\n",
      "Epoch 19, Loss: 5.409675545096397, Accuracy: 0.10154144830070436\n",
      "Epoch 20, Loss: 5.395259305238723, Accuracy: 0.10197041474748403\n",
      "Epoch 21, Loss: 5.381851180195809, Accuracy: 0.10332887233933434\n",
      "Epoch 22, Loss: 5.36696617603302, Accuracy: 0.10264491540379822\n",
      "Epoch 23, Loss: 5.354254177212715, Accuracy: 0.10453254843596368\n",
      "Epoch 24, Loss: 5.340330655574799, Accuracy: 0.10541063292417675\n",
      "Epoch 25, Loss: 5.330546081066132, Accuracy: 0.10571712175384164\n",
      "Epoch 26, Loss: 5.32132155418396, Accuracy: 0.10646606476046144\n",
      "Epoch 27, Loss: 5.31181804895401, Accuracy: 0.10686496712267399\n",
      "Epoch 28, Loss: 5.3017320775985715, Accuracy: 0.1073816150939092\n",
      "Epoch 29, Loss: 5.2921700239181515, Accuracy: 0.10793725736439228\n",
      "Epoch 30, Loss: 5.283350484371185, Accuracy: 0.10769713297951966\n",
      "Epoch 31, Loss: 5.274187573194504, Accuracy: 0.10863501269370318\n",
      "Epoch 32, Loss: 5.270089502930642, Accuracy: 0.10946698908694089\n",
      "Epoch 33, Loss: 5.258874013423919, Accuracy: 0.10971346656326204\n",
      "Epoch 34, Loss: 5.252426772117615, Accuracy: 0.10979103504680097\n",
      "Epoch 35, Loss: 5.245607489943504, Accuracy: 0.11088559010066092\n",
      "Epoch 36, Loss: 5.2376376664638515, Accuracy: 0.11106699467636645\n",
      "Epoch 37, Loss: 5.233280488848687, Accuracy: 0.11092760073021055\n",
      "Epoch 38, Loss: 5.226178851127624, Accuracy: 0.1115856333449483\n",
      "Epoch 39, Loss: 5.220077354907989, Accuracy: 0.11226242951117456\n",
      "Epoch 40, Loss: 5.213513501882553, Accuracy: 0.11270699448883534\n",
      "Epoch 41, Loss: 5.211492195129394, Accuracy: 0.11252231876831502\n",
      "Epoch 42, Loss: 5.20327870130539, Accuracy: 0.1124660741724074\n",
      "Epoch 43, Loss: 5.198137564659119, Accuracy: 0.11450418536085635\n",
      "Epoch 44, Loss: 5.19351848423481, Accuracy: 0.11472362719709053\n",
      "Epoch 45, Loss: 5.18886360168457, Accuracy: 0.11377962446771563\n",
      "Epoch 46, Loss: 5.1828219324350355, Accuracy: 0.11419361216016113\n",
      "Epoch 47, Loss: 5.178333505392074, Accuracy: 0.11531010082457215\n",
      "Epoch 48, Loss: 5.176887453794479, Accuracy: 0.11561315446626395\n",
      "Epoch 49, Loss: 5.168156945705414, Accuracy: 0.11591530153993518\n",
      "Epoch 50, Loss: 5.162686069011688, Accuracy: 0.1160336541570723\n",
      "Epoch 51, Loss: 5.1574864208698274, Accuracy: 0.11547695012763143\n",
      "Epoch 52, Loss: 5.15713706612587, Accuracy: 0.11604610921349376\n",
      "Epoch 53, Loss: 5.15519813477993, Accuracy: 0.11631466477178037\n",
      "Epoch 54, Loss: 5.149037272930145, Accuracy: 0.11696758154779673\n",
      "Epoch 55, Loss: 5.147051107287407, Accuracy: 0.1169084187084809\n",
      "Epoch 56, Loss: 5.1404708510637285, Accuracy: 0.11709753736387939\n",
      "Epoch 57, Loss: 5.1446865087747575, Accuracy: 0.11689129509497434\n",
      "Epoch 58, Loss: 5.134886057376861, Accuracy: 0.11713949447032064\n",
      "Epoch 59, Loss: 5.13123414337635, Accuracy: 0.11773125422652811\n",
      "Epoch 60, Loss: 5.1286425995826725, Accuracy: 0.11812367653474212\n",
      "Epoch 61, Loss: 5.126912502050399, Accuracy: 0.11857373429462313\n",
      "Epoch 62, Loss: 5.122928961515426, Accuracy: 0.11807312747463584\n",
      "Epoch 63, Loss: 5.120996349453926, Accuracy: 0.11872894480358809\n",
      "Epoch 64, Loss: 5.118098011016846, Accuracy: 0.11932709088083357\n",
      "Epoch 65, Loss: 5.111500822305679, Accuracy: 0.11921925928909331\n",
      "Epoch 66, Loss: 5.10950925707817, Accuracy: 0.119275849214755\n",
      "Epoch 67, Loss: 5.106466008424759, Accuracy: 0.11960654955357314\n",
      "Epoch 68, Loss: 5.1065235286951065, Accuracy: 0.11922419012058526\n",
      "Epoch 69, Loss: 5.0980623406171794, Accuracy: 0.12066728887613863\n",
      "Epoch 70, Loss: 5.0969173032045365, Accuracy: 0.12093006048817187\n",
      "Epoch 71, Loss: 5.097580998539924, Accuracy: 0.1201458726124838\n",
      "Epoch 72, Loss: 5.094474921822548, Accuracy: 0.12053474722895771\n",
      "Epoch 73, Loss: 5.08944125533104, Accuracy: 0.12056678155437112\n",
      "Epoch 74, Loss: 5.0835490101575855, Accuracy: 0.12151913816574961\n",
      "Epoch 75, Loss: 5.085476841330529, Accuracy: 0.12113511067815125\n",
      "Epoch 76, Loss: 5.082716023921966, Accuracy: 0.12090428251773119\n",
      "Epoch 77, Loss: 5.076015108823777, Accuracy: 0.12192005683667957\n",
      "Epoch 78, Loss: 5.075282582640648, Accuracy: 0.12221384351607412\n",
      "Epoch 79, Loss: 5.071588874459267, Accuracy: 0.1215665126638487\n",
      "Epoch 80, Loss: 5.06842074394226, Accuracy: 0.12257442945614457\n",
      "Epoch 81, Loss: 5.070240507125854, Accuracy: 0.12180727215483785\n",
      "Epoch 82, Loss: 5.06547050178051, Accuracy: 0.12305725118611008\n",
      "Epoch 83, Loss: 5.064275306463242, Accuracy: 0.12235330786556005\n",
      "Epoch 84, Loss: 5.0578043222427365, Accuracy: 0.12301520919892937\n",
      "Epoch 85, Loss: 5.059863706231117, Accuracy: 0.12290496710687876\n",
      "Epoch 86, Loss: 5.056786626577377, Accuracy: 0.12252976450603455\n",
      "Epoch 87, Loss: 5.055985215306282, Accuracy: 0.12378043205011637\n",
      "Epoch 88, Loss: 5.0480909764766695, Accuracy: 0.12383149235043675\n",
      "Epoch 89, Loss: 5.050584133267403, Accuracy: 0.12360680960118771\n",
      "Epoch 90, Loss: 5.0435499340295795, Accuracy: 0.12342251391150057\n",
      "Epoch 91, Loss: 5.039306548833847, Accuracy: 0.12432855970691889\n",
      "Epoch 92, Loss: 5.04410877585411, Accuracy: 0.12414391700644047\n",
      "Epoch 93, Loss: 5.039587567448616, Accuracy: 0.1247356421733275\n",
      "Epoch 94, Loss: 5.04062597155571, Accuracy: 0.12424294346477836\n",
      "Epoch 95, Loss: 5.040672453641892, Accuracy: 0.12408986209891737\n",
      "Epoch 96, Loss: 5.0342575532197955, Accuracy: 0.12460135219153017\n",
      "Epoch 97, Loss: 5.033039515018463, Accuracy: 0.12419442678336054\n",
      "Epoch 98, Loss: 5.029937804937362, Accuracy: 0.12461211276240647\n",
      "Epoch 99, Loss: 5.027780656814575, Accuracy: 0.12476830134168267\n",
      "Epoch 100, Loss: 5.025973134040832, Accuracy: 0.12500119116622954\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = nn.Embedding(vocab_size, embedding_dim).to(device)\n",
    "#model = Model().to(device)\n",
    "PAD_IDX = vocab_to_index.get(\"<pad>\", 0)  # Ensure this is consistent with your vocab\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Assuming: tokenized_lines = open(\"input.txt\").readlines(), vocab_to_idx built\n",
    "dataset = ShakespeareDataset(tokenized_lines, vocab_to_index)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "def create_padding_mask(input_ids, pad_idx):\n",
    "    input_ids: (batch, seq_len)\n",
    "    return (input_ids != pad_idx).unsqueeze(1)  # (batch, 1, seq_len)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "\n",
    "    for inputs, targets in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Get embeddings\n",
    "        input_embeddings = embedding_layer(inputs)   \n",
    "        pos_enc = positional_encodings(input_embeddings.size(1), embedding_dim, device)\n",
    "        input_with_pos = input_embeddings + pos_enc\n",
    "\n",
    "        logits = model(input_with_pos, input_with_pos, input_with_pos, input_with_pos)\n",
    "        \n",
    "        loss = loss_fn(logits.view(-1, vocab_size), targets.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accuracy\n",
    "        predicted = torch.argmax(logits, dim=-1)\n",
    "        correct = (predicted == targets).float()\n",
    "        mask = (targets != PAD_IDX).float()\n",
    "        accuracy = (correct * mask).sum() / mask.sum()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_accuracy += accuracy.item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    avg_accuracy = total_accuracy / len(loader)\n",
    "    #print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.4f}\")\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss}, Accuracy: {avg_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.00095293298364"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([20827, 12845,  9796, 13784,  1096,     0,     0,     0,     0,     0])\n",
      "Target: tensor([12845,  9796, 13784,  1096, 22793,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "for x, y in loader:\n",
    "    print(\"Input:\", x[0])\n",
    "    print(\"Target:\", y[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-13.9347, -13.9003, -14.0614,  ..., -14.1253, -13.8269, -13.9101],\n",
       "         [-14.0451, -14.0219, -14.0426,  ..., -14.0494, -14.0346, -13.9947],\n",
       "         [-16.2973, -16.1445, -16.1669,  ..., -16.0002, -16.0649, -15.7586],\n",
       "         ...,\n",
       "         [-16.3596, -15.6487, -16.2291,  ..., -16.0888, -16.0513, -16.0338],\n",
       "         [-16.4813, -15.7773, -16.3391,  ..., -16.1528, -16.1347, -16.2208],\n",
       "         [-15.8865, -15.2421, -15.6237,  ..., -15.5098, -15.5916, -15.6625]],\n",
       "\n",
       "        [[-17.5027, -17.2879, -17.5936,  ..., -17.2092, -17.2924, -17.2838],\n",
       "         [-15.2717, -15.1283, -15.3442,  ..., -15.3042, -15.2778, -15.3514],\n",
       "         [-22.8034, -22.4736, -22.3140,  ..., -22.6517, -22.4188, -22.6989],\n",
       "         ...,\n",
       "         [-13.1163, -13.0389, -13.1636,  ..., -13.0648, -13.1783, -13.0091],\n",
       "         [-14.3729, -14.0295, -14.3412,  ..., -14.1798, -14.4101, -14.3460],\n",
       "         [-14.3535, -14.0786, -14.2954,  ..., -14.1355, -14.4220, -14.4050]],\n",
       "\n",
       "        [[-12.0414, -11.8074, -11.6629,  ..., -11.7380, -11.8163, -11.6790],\n",
       "         [-16.9871, -16.3988, -16.7642,  ..., -16.7983, -16.4212, -16.4439],\n",
       "         [-16.8285, -16.0103, -16.8635,  ..., -16.1960, -16.2061, -16.1627],\n",
       "         ...,\n",
       "         [-16.3596, -15.6487, -16.2291,  ..., -16.0888, -16.0513, -16.0338],\n",
       "         [-16.4813, -15.7773, -16.3391,  ..., -16.1528, -16.1347, -16.2208],\n",
       "         [-16.5813, -15.9123, -16.3627,  ..., -16.1906, -16.2399, -16.3807]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-13.9190, -13.7242, -13.9999,  ..., -14.0054, -13.9132, -13.8078],\n",
       "         [-15.0422, -14.9542, -15.0330,  ..., -14.9798, -14.6848, -15.0339],\n",
       "         [-12.5115, -12.5879, -12.6206,  ..., -12.5208, -12.5084, -12.4712],\n",
       "         ...,\n",
       "         [-15.6099, -15.0289, -15.5463,  ..., -15.3684, -15.4235, -15.4108],\n",
       "         [-15.7581, -15.1913, -15.6981,  ..., -15.4586, -15.5371, -15.6429],\n",
       "         [-15.7746, -15.2887, -15.6971,  ..., -15.4212, -15.5895, -15.7928]],\n",
       "\n",
       "        [[-15.5938, -15.2487, -15.5921,  ..., -15.4606, -15.5792, -15.4431],\n",
       "         [-14.8633, -14.7587, -14.9473,  ..., -14.8535, -14.8574, -14.8639],\n",
       "         [-14.2215, -14.5569, -14.3957,  ..., -14.2228, -14.3211, -14.2993],\n",
       "         ...,\n",
       "         [-16.3596, -15.6487, -16.2291,  ..., -16.0888, -16.0513, -16.0338],\n",
       "         [-16.4541, -15.7636, -16.3299,  ..., -16.1270, -16.1149, -16.2167],\n",
       "         [-16.5130, -15.8978, -16.3714,  ..., -16.1265, -16.2013, -16.4108]],\n",
       "\n",
       "        [[-14.0171, -14.0167, -14.1049,  ..., -14.1189, -14.0285, -13.9527],\n",
       "         [-15.6976, -15.5273, -15.7794,  ..., -15.5484, -15.6644, -15.6971],\n",
       "         [-15.6221, -15.5189, -15.7543,  ..., -15.6521, -15.7304, -15.6080],\n",
       "         ...,\n",
       "         [-14.6937, -14.2578, -14.6608,  ..., -14.5050, -14.6639, -14.5790],\n",
       "         [-14.9055, -14.4767, -14.8742,  ..., -14.6565, -14.8369, -14.8709],\n",
       "         [-14.3535, -14.0786, -14.2954,  ..., -14.1355, -14.4220, -14.4050]]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "<start> tale sir, a careful height will be absent. wend of thy name. charge. caps thing; eye, cause procures with old tale, help. times and yet most piteous woes hung long, who's here! provost, thinkest glory. and that name became is't possible friend of sorrow wind betwixt as i said, dearly\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def generate_sequence(model, start_text, vocab_to_idx, idx_to_vocab, embedding_layer, device, max_len=50):\n",
    "    model.eval()  # Evaluation mode\n",
    "    start_tokens = start_text.lower().split()\n",
    "\n",
    "    # Convert words to indices\n",
    "    input_ids = [vocab_to_idx.get(word, vocab_to_idx[\"<pad>\"]) for word in start_tokens]\n",
    "    generated = torch.tensor(input_ids, dtype=torch.long, device=device).unsqueeze(0)  # (1, seq_len)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        seq_len = generated.size(1)\n",
    "\n",
    "        # Recalculate positional encodings each time\n",
    "        pos = positional_encodings(seq_len, embedding_layer.embedding_dim, device)\n",
    "        input_embed = embedding_layer(generated) + pos\n",
    "\n",
    "        # Attention mask\n",
    "        attn_mask = create_padding_mask(generated, vocab_to_idx[\"<pad>\"]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q = k = v = input_embed\n",
    "            logits = model(q, k, v, input_embed, attn_mask)\n",
    "\n",
    "        # Sample next token\n",
    "        logits = logits[:, -1, :]  # Get last token's logits\n",
    "        temperature = 0.7\n",
    "        probs = torch.softmax(logits / temperature, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)  # Shape: (1, 1)\n",
    "\n",
    "        # Stop if end token\n",
    "        token_id = next_token.item()\n",
    "        if idx_to_vocab.get(token_id, \"\") == \"<end>\":\n",
    "            break\n",
    "\n",
    "        # Append next token\n",
    "        generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "    # Convert generated indices back to words\n",
    "    generated_text = ' '.join([idx_to_vocab.get(idx.item(), \"<unk>\") for idx in generated.squeeze()])\n",
    "    return generated_text\n",
    "\n",
    "# Example of inference usage:\n",
    "start_text = \"<start>\"  # Starting text for generation\n",
    "generated_text = generate_sequence(\n",
    "    model=model, \n",
    "    start_text=start_text, \n",
    "    vocab_to_idx=vocab_to_index, \n",
    "    idx_to_vocab={index: word for word, index in vocab_to_index.items()}, \n",
    "    embedding_layer=embedding_layer, \n",
    "    device=device,\n",
    "    max_len=50  # Limit generated sequence length\n",
    ")\n",
    "\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
