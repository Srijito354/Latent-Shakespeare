{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_lines = open(\"input.txt\", \"r\")\n",
    "tokenized_lines = tokenized_lines.readlines()\n",
    "\n",
    "vocab = set()\n",
    "special_tokens = [\"<pad>\", \"<start>\", \"<end>\"]\n",
    "for sentence in tokenized_lines:\n",
    "    vocab.update(sentence.split())\n",
    "vocab = special_tokens + list(vocab)\n",
    "\n",
    "vocab_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "vocab_size = len(vocab)\n",
    "#print(vocab)\n",
    "#print(\"Vocab size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "PAD_IDX = vocab_to_index[PAD_TOKEN]\n",
    "\n",
    "def collate_batch(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "\n",
    "    #inputs = [torch.tensor(seq, dtype = torch.long()) for seq in inputs]\n",
    "    #targets = [torch.tensor(seq, dtype = torch.long()) for seq in targets]\n",
    "\n",
    "    padded_inputs = pad_sequence(inputs, batch_first=True, padding_value=PAD_IDX)\n",
    "    padded_targets = pad_sequence(targets, batch_first=True, padding_value=PAD_IDX)\n",
    "\n",
    "    return padded_inputs, padded_targets\n",
    "\n",
    "\n",
    "# 1. Rebuild vocab from lowercased text and include <unk>\n",
    "special_tokens = [\"<pad>\", \"<start>\", \"<end>\", \"<unk>\"]\n",
    "\n",
    "vocab_to_index = {}\n",
    "\n",
    "vocab = set()\n",
    "for sentence in tokenized_lines:\n",
    "    vocab.update(sentence.lower().split())      # lowercase here\n",
    "\n",
    "vocab = special_tokens + sorted(vocab)          # sorted for reproducibility\n",
    "vocab_to_index = {w:i for i,w in enumerate(vocab)}\n",
    "\n",
    "PAD_IDX = vocab_to_index[\"<pad>\"]\n",
    "UNK_IDX = vocab_to_index[\"<unk>\"]\n",
    "\n",
    "# 2. Update your Dataset to use .get(â€¦, UNK_IDX) instead of direct indexing\n",
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, tokenized_lines, vocab_to_idx):\n",
    "        self.data = [\n",
    "            line.lower().split()\n",
    "            for line in tokenized_lines\n",
    "            if len(line.lower().split()) > 2  # ignore short lines\n",
    "        ]\n",
    "        self.vocab_to_idx = vocab_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        words = self.data[idx]\n",
    "\n",
    "        # THIS should raise error if token is missing\n",
    "        input_ids = [self.vocab_to_idx.get(word, self.vocab_to_idx[\"<unk>\"]) for word in words[:-1]]\n",
    "        target_ids = [self.vocab_to_idx.get(word, self.vocab_to_idx[\"<unk>\"]) for word in words[1:]]\n",
    "\n",
    "        return torch.tensor(input_ids, dtype=torch.long), torch.tensor(target_ids, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encodings(seq_len, embedding_dim, device):\n",
    "    position = torch.arange(seq_len, dtype=torch.float, device=device).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, embedding_dim, 2, device=device).float() * (-math.log(10000.0) / embedding_dim))\n",
    "    pe = torch.zeros(seq_len, embedding_dim, device=device)\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(self_attention, self).__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask=None):\n",
    "        # Q, K, V shape: (batch, seq_len, dim)\n",
    "        batch_size, seq_len, dim = Q.size()\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(dim)  # (batch, seq_len, seq_len)\n",
    "\n",
    "        # Causal mask (upper triangular)\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=Q.device) * float('-inf'), diagonal=1)\n",
    "        scores = scores + causal_mask\n",
    "\n",
    "        # Padding mask (optional)\n",
    "        if attn_mask is not None:\n",
    "            # attn_mask: (batch, 1, seq_len), 1 for keep, 0 for mask\n",
    "            scores = scores.masked_fill(attn_mask == 0, float('-inf'))\n",
    "\n",
    "        weights = self.softmax(scores)\n",
    "        context = torch.matmul(weights, V)  # (batch, seq_len, dim)\n",
    "\n",
    "        return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # creating the multi-headed attention block.\n",
    "        self.self_attn1 = self_attention()\n",
    "        self.self_attn2 = self_attention()\n",
    "        self.self_attn3 = self_attention()\n",
    "        self.self_attn4 = self_attention()\n",
    "        self.self_attn5 = self_attention()\n",
    "        self.self_attn6 = self_attention()\n",
    "        self.self_attn7 = self_attention()\n",
    "        self.self_attn8 = self_attention()\n",
    "\n",
    "        self.self_attn9 = self_attention()\n",
    "        self.self_attn10 = self_attention()\n",
    "        self.self_attn11 = self_attention()\n",
    "        self.self_attn12 = self_attention()\n",
    "        self.self_attn13 = self_attention()\n",
    "        self.self_attn14 = self_attention()\n",
    "        self.self_attn15 = self_attention()\n",
    "        self.self_attn16 = self_attention()\n",
    "\n",
    "\n",
    "        # All the layers, we gonna need to make the decoder work.\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.softmax = nn.Softmax(-1)\n",
    "        \n",
    "        self.latent_downscale = nn.Linear(embedding_dim, 32)\n",
    "        self.latent_upscale = nn.Linear(32, embedding_dim)\n",
    "\n",
    "        self.final_linear_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, Q, K, V, X, attn_mask=None):\n",
    "        q = self.latent_downscale(Q)\n",
    "        k = self.latent_downscale(K)\n",
    "        v = self.latent_downscale(V)\n",
    "        x = self.latent_downscale(X)\n",
    "\n",
    "        context1 = self.self_attn1(q, k, v, attn_mask)\n",
    "        context2 = self.self_attn2(q, k, v, attn_mask)\n",
    "        context3 = self.self_attn3(q, k, v, attn_mask)\n",
    "        context4 = self.self_attn4(q, k, v, attn_mask)\n",
    "        context5 = self.self_attn5(q, k, v, attn_mask)\n",
    "        context6 = self.self_attn6(q, k, v, attn_mask)\n",
    "        context7 = self.self_attn7(q, k, v, attn_mask)\n",
    "        context8 = self.self_attn8(q, k, v, attn_mask)\n",
    "\n",
    "        context9 = self.self_attn9(q, k, v, attn_mask)\n",
    "        context10 = self.self_attn10(q, k, v, attn_mask)\n",
    "        context11 = self.self_attn11(q, k, v, attn_mask)\n",
    "        context12 = self.self_attn12(q, k, v, attn_mask)\n",
    "        context13 = self.self_attn13(q, k, v, attn_mask)\n",
    "        context14 = self.self_attn14(q, k, v, attn_mask)\n",
    "        context15 = self.self_attn15(q, k, v, attn_mask)\n",
    "        context16 = self.self_attn16(q, k, v, attn_mask)\n",
    "\n",
    "        combined = torch.cat((context1, context2, context3, context4, context5, context6, context7, context8, context9, context10, context11, context12, context13, context14, context15, context16), 2)\n",
    "        final_encodings = combined + self.latent_upscale(x)\n",
    "        final_encodings = self.layer_norm(final_encodings)\n",
    "        #logits = self.final_linear_layer(final_encodings)\n",
    "\n",
    "        return final_encodings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 10.202516989707947, Accuracy: 3.513076633680612e-05\n",
      "Epoch 2, Loss: 10.177374634742737, Accuracy: 3.0723290401510893e-05\n",
      "Epoch 3, Loss: 10.166835522651672, Accuracy: 2.3544593714177607e-05\n",
      "Epoch 4, Loss: 10.165000542402268, Accuracy: 7.202249369584024e-05\n",
      "Epoch 5, Loss: 10.16396257162094, Accuracy: 3.5908641293644906e-05\n",
      "Epoch 6, Loss: 10.162994463443756, Accuracy: 4.119650402572006e-05\n",
      "Epoch 7, Loss: 10.162463293075561, Accuracy: 3.511643677484244e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m final_encodings \u001b[38;5;241m=\u001b[39m model(input_with_pos, input_with_pos, input_with_pos, input_with_pos)\n\u001b[1;32m     34\u001b[0m final_encodings \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(final_encodings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], store_res)\u001b[38;5;241m.\u001b[39mto(device)(final_encodings\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 35\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)(final_encodings\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     37\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size), targets\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     38\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:112\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:118\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m     \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkaiming_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m         fan_in, _ \u001b[38;5;241m=\u001b[39m init\u001b[38;5;241m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/init.py:518\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity, generator)\u001b[0m\n\u001b[1;32m    516\u001b[0m bound \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m3.0\u001b[39m) \u001b[38;5;241m*\u001b[39m std  \u001b[38;5;66;03m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embedding_layer = nn.Embedding(vocab_size, embedding_dim).to(device)\n",
    "#model = Model().to(device)\n",
    "PAD_IDX = vocab_to_index.get(\"<pad>\", 0)  # Ensure this is consistent with your vocab\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Assuming: tokenized_lines = open(\"input.txt\").readlines(), vocab_to_idx built\n",
    "dataset = ShakespeareDataset(tokenized_lines, vocab_to_index)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "def create_padding_mask(input_ids, pad_idx):\n",
    "    input_ids: (batch, seq_len)\n",
    "    return (input_ids != pad_idx).unsqueeze(1)  # (batch, 1, seq_len)\n",
    "\n",
    "for epoch in range(100000):\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "\n",
    "    for inputs, targets in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Get embeddings\n",
    "        input_embeddings = embedding_layer(inputs)   \n",
    "        pos_enc = positional_encodings(input_embeddings.size(1), embedding_dim, device)\n",
    "        input_with_pos = input_embeddings + pos_enc\n",
    "\n",
    "        store_res = input_with_pos.shape[1]\n",
    "\n",
    "        input_with_pos = nn.Linear(input_with_pos.shape[1], 8).to(device)(input_with_pos.transpose(-2, -1))\n",
    "        input_with_pos = input_with_pos.transpose(-2, -1)\n",
    "\n",
    "        final_encodings = model(input_with_pos, input_with_pos, input_with_pos, input_with_pos)\n",
    "        final_encodings = nn.Linear(final_encodings.shape[1], store_res).to(device)(final_encodings.transpose(-2, -1))\n",
    "        logits = nn.Linear(embedding_dim, vocab_size).to(device)(final_encodings.transpose(-2, -1))\n",
    "        \n",
    "        loss = loss_fn(logits.view(-1, vocab_size), targets.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accuracy\n",
    "        predicted = torch.argmax(logits, dim=-1)\n",
    "        correct = (predicted == targets).float()\n",
    "        mask = (targets != PAD_IDX).float()\n",
    "        accuracy = (correct * mask).sum() / mask.sum()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_accuracy += accuracy.item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    avg_accuracy = total_accuracy / len(loader)\n",
    "    #print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.4f}\")\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss}, Accuracy: {avg_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.00095293298364"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([20827, 12845,  9796, 13784,  1096,     0,     0,     0,     0,     0])\n",
      "Target: tensor([12845,  9796, 13784,  1096, 22793,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "for x, y in loader:\n",
    "    print(\"Input:\", x[0])\n",
    "    print(\"Target:\", y[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "<start> tale sir, a careful height will be absent. wend of thy name. charge. caps thing; eye, cause procures with old tale, help. times and yet most piteous woes hung long, who's here! provost, thinkest glory. and that name became is't possible friend of sorrow wind betwixt as i said, dearly\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def generate_sequence(model, start_text, vocab_to_idx, idx_to_vocab, embedding_layer, device, max_len=50):\n",
    "    model.eval()  # Evaluation mode\n",
    "    start_tokens = start_text.lower().split()\n",
    "\n",
    "    # Convert words to indices\n",
    "    input_ids = [vocab_to_idx.get(word, vocab_to_idx[\"<pad>\"]) for word in start_tokens]\n",
    "    generated = torch.tensor(input_ids, dtype=torch.long, device=device).unsqueeze(0)  # (1, seq_len)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        seq_len = generated.size(1)\n",
    "\n",
    "        # Recalculate positional encodings each time\n",
    "        pos = positional_encodings(seq_len, embedding_layer.embedding_dim, device)\n",
    "        input_embed = embedding_layer(generated) + pos\n",
    "\n",
    "        # Attention mask\n",
    "        attn_mask = create_padding_mask(generated, vocab_to_idx[\"<pad>\"]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q = k = v = input_embed\n",
    "            logits = model(q, k, v, input_embed, attn_mask)\n",
    "\n",
    "        # Sample next token\n",
    "        logits = logits[:, -1, :]  # Get last token's logits\n",
    "        temperature = 0.7\n",
    "        probs = torch.softmax(logits / temperature, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)  # Shape: (1, 1)\n",
    "\n",
    "        # Stop if end token\n",
    "        token_id = next_token.item()\n",
    "        if idx_to_vocab.get(token_id, \"\") == \"<end>\":\n",
    "            break\n",
    "\n",
    "        # Append next token\n",
    "        generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "    # Convert generated indices back to words\n",
    "    generated_text = ' '.join([idx_to_vocab.get(idx.item(), \"<unk>\") for idx in generated.squeeze()])\n",
    "    return generated_text\n",
    "\n",
    "# Example of inference usage:\n",
    "start_text = \"<start>\"  # Starting text for generation\n",
    "generated_text = generate_sequence(\n",
    "    model=model, \n",
    "    start_text=start_text, \n",
    "    vocab_to_idx=vocab_to_index, \n",
    "    idx_to_vocab={index: word for word, index in vocab_to_index.items()}, \n",
    "    embedding_layer=embedding_layer, \n",
    "    device=device,\n",
    "    max_len=50  # Limit generated sequence length\n",
    ")\n",
    "\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
