{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_lines = open(\"input.txt\", \"r\")\n",
    "tokenized_lines = tokenized_lines.readlines()\n",
    "\n",
    "vocab = set()\n",
    "special_tokens = [\"<pad>\", \"<start>\", \"<end>\"]\n",
    "for sentence in tokenized_lines:\n",
    "    vocab.update(sentence.split())\n",
    "vocab = special_tokens + list(vocab)\n",
    "\n",
    "vocab_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "vocab_size = len(vocab)\n",
    "#print(vocab)\n",
    "#print(\"Vocab size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "PAD_IDX = vocab_to_index[PAD_TOKEN]\n",
    "\n",
    "def collate_batch(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "\n",
    "    #inputs = [torch.tensor(seq, dtype = torch.long()) for seq in inputs]\n",
    "    #targets = [torch.tensor(seq, dtype = torch.long()) for seq in targets]\n",
    "\n",
    "    padded_inputs = pad_sequence(inputs, batch_first=True, padding_value=PAD_IDX)\n",
    "    padded_targets = pad_sequence(targets, batch_first=True, padding_value=PAD_IDX)\n",
    "\n",
    "    return padded_inputs, padded_targets\n",
    "\n",
    "\n",
    "# 1. Rebuild vocab from lowercased text and include <unk>\n",
    "special_tokens = [\"<pad>\", \"<start>\", \"<end>\", \"<unk>\"]\n",
    "\n",
    "vocab_to_index = {}\n",
    "\n",
    "vocab = set()\n",
    "for sentence in tokenized_lines:\n",
    "    vocab.update(sentence.lower().split())      # lowercase here\n",
    "\n",
    "vocab = special_tokens + sorted(vocab)          # sorted for reproducibility\n",
    "vocab_to_index = {w:i for i,w in enumerate(vocab)}\n",
    "\n",
    "PAD_IDX = vocab_to_index[\"<pad>\"]\n",
    "UNK_IDX = vocab_to_index[\"<unk>\"]\n",
    "\n",
    "# 2. Update your Dataset to use .get(â€¦, UNK_IDX) instead of direct indexing\n",
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, tokenized_lines, vocab_to_idx):\n",
    "        self.data = [\n",
    "            line.lower().split()\n",
    "            for line in tokenized_lines\n",
    "            if len(line.lower().split()) > 2  # ignore short lines\n",
    "        ]\n",
    "        self.vocab_to_idx = vocab_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        words = self.data[idx]\n",
    "\n",
    "        # THIS should raise error if token is missing\n",
    "        input_ids = [self.vocab_to_idx.get(word, self.vocab_to_idx[\"<unk>\"]) for word in words[:-1]]\n",
    "        target_ids = [self.vocab_to_idx.get(word, self.vocab_to_idx[\"<unk>\"]) for word in words[1:]]\n",
    "\n",
    "        return torch.tensor(input_ids, dtype=torch.long), torch.tensor(target_ids, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndata = [\\n    line.lower().split()\\n    for line in tokenized_lines\\n    if len(line.lower().split()) > 2  # ignore short lines\\n]\\nfor i in range(5):\\n    words = data[i]\\n    print(words[:-1])\\n    print(words[1:])\\n'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "data = [\n",
    "    line.lower().split()\n",
    "    for line in tokenized_lines\n",
    "    if len(line.lower().split()) > 2  # ignore short lines\n",
    "]\n",
    "for i in range(5):\n",
    "    words = data[i]\n",
    "    print(words[:-1])\n",
    "    print(words[1:])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encodings(seq_len, embedding_dim, device):\n",
    "    position = torch.arange(seq_len, dtype=torch.float, device=device).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, embedding_dim, 2, device=device).float() * (-math.log(10000.0) / embedding_dim))\n",
    "    pe = torch.zeros(seq_len, embedding_dim, device=device)\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nclass self_attention(nn.Module):\\n    def __init__(self):\\n        super(self_attention, self).__init__()\\n        #self.w_qkv_downscale = nn.Linear(in_channels=16, out_channels=2)\\n        #self.latent_upscale = nn.Linear(in_channels=2, out_channels=16)\\n        #self.layer_norm = nn.LayerNorm()\\n        self.softmax = nn.Softmax()\\n\\n    def forward(self, Q, K, V):\\n        #Q = self.w_qkv_downscale(Q)\\n        #K = self.w_qkv_downscale(K)\\n        #V = self.w_qkv_downscale(V)\\n        seq_len = Q.size(1)\\n        mask = torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1)\\n        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (16 ** 0.5) + mask\\n        #attention_weights = self.softmax(attention_scores, dim=-1)\\n        attention_weights = self.softmax(attention_scores)\\n        context = torch.matmul(attention_weights, V)\\n\\n        #context = self.latent_upscale(context)\\n\\n        # Residual + Norm\\n        # x = self.layer_norm(context + x)\\n\\n        # Feedforward + Norm\\n        #ff_out = self.feed_fwd(x)\\n        #out = self.layer_norm(ff_out + x)\\n\\n        # Final linear (optional)\\n        #return self.output_proj(out)\\n        return context\\n\""
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "class self_attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(self_attention, self).__init__()\n",
    "        #self.w_qkv_downscale = nn.Linear(in_channels=16, out_channels=2)\n",
    "        #self.latent_upscale = nn.Linear(in_channels=2, out_channels=16)\n",
    "        #self.layer_norm = nn.LayerNorm()\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        #Q = self.w_qkv_downscale(Q)\n",
    "        #K = self.w_qkv_downscale(K)\n",
    "        #V = self.w_qkv_downscale(V)\n",
    "        seq_len = Q.size(1)\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1)\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (16 ** 0.5) + mask\n",
    "        #attention_weights = self.softmax(attention_scores, dim=-1)\n",
    "        attention_weights = self.softmax(attention_scores)\n",
    "        context = torch.matmul(attention_weights, V)\n",
    "\n",
    "        #context = self.latent_upscale(context)\n",
    "\n",
    "        # Residual + Norm\n",
    "        # x = self.layer_norm(context + x)\n",
    "\n",
    "        # Feedforward + Norm\n",
    "        #ff_out = self.feed_fwd(x)\n",
    "        #out = self.layer_norm(ff_out + x)\n",
    "\n",
    "        # Final linear (optional)\n",
    "        #return self.output_proj(out)\n",
    "        return context\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(self_attention, self).__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask=None):\n",
    "        # Q, K, V shape: (batch, seq_len, dim)\n",
    "        batch_size, seq_len, dim = Q.size()\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(dim)  # (batch, seq_len, seq_len)\n",
    "\n",
    "        # Causal mask (upper triangular)\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=Q.device) * float('-inf'), diagonal=1)\n",
    "        scores = scores + causal_mask\n",
    "\n",
    "        # Padding mask (optional)\n",
    "        if attn_mask is not None:\n",
    "            # attn_mask: (batch, 1, seq_len), 1 for keep, 0 for mask\n",
    "            scores = scores.masked_fill(attn_mask == 0, float('-inf'))\n",
    "\n",
    "        weights = self.softmax(scores)\n",
    "        context = torch.matmul(weights, V)  # (batch, seq_len, dim)\n",
    "\n",
    "        return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass Model(nn.Module):\\n    def __init__(self):\\n        super(Model, self).__init__()\\n        \\n        # creating the multi-headed attention block.\\n        self.self_attn1 = self_attention()\\n        self.self_attn2 = self_attention()\\n        self.self_attn3 = self_attention()\\n        self.self_attn4 = self_attention()\\n\\n        self.self_attn5 = self_attention()\\n        self.self_attn6 = self_attention()\\n        self.self_attn7 = self_attention()\\n        self.self_attn8 = self_attention()\\n\\n\\n        # All the layers, we gonna need to make the decoder work.\\n        self.layer_norm = nn.LayerNorm(16)\\n        self.softmax = nn.Softmax(-1)\\n\\n        self.latent_downscale = nn.Linear(16, 2)\\n        self.latent_upscale = nn.Linear(2, 16)\\n\\n        self.final_linear_layer = nn.Linear(16, vocab_size) # out_features can be replaced with embedding dimension (at least, here).\\n\\n\\n    def forward(self, Q, K, V, X):\\n\\n        q = self.latent_downscale(Q)\\n        k = self.latent_downscale(K)\\n        v = self.latent_downscale(V)\\n\\n        x = self.latent_downscale(X)\\n        \\n        # getting the contexts from the respective self attention layers in the multi-headed attention block.\\n        context1 = self.self_attn1(q, k, v)\\n        context2 = self.self_attn2(q, k, v)\\n        context3 = self.self_attn3(q, k, v)\\n        context4 = self.self_attn4(q, k, v)\\n\\n        context5 = self.self_attn5(q, k, v)\\n        context6 = self.self_attn6(q, k, v)\\n        context7 = self.self_attn7(q, k, v)\\n        context8 = self.self_attn8(q, k, v)\\n\\n        # adding them up\\n        final_encodings = self.latent_upscale(context1 + context2 + context3 + context5 + context6 + context7 + context8 + x)\\n        final_encodings = self.layer_norm(final_encodings)\\n        logits = self.final_linear_layer(final_encodings)\\n\\n        return logits\\n'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # creating the multi-headed attention block.\n",
    "        self.self_attn1 = self_attention()\n",
    "        self.self_attn2 = self_attention()\n",
    "        self.self_attn3 = self_attention()\n",
    "        self.self_attn4 = self_attention()\n",
    "\n",
    "        self.self_attn5 = self_attention()\n",
    "        self.self_attn6 = self_attention()\n",
    "        self.self_attn7 = self_attention()\n",
    "        self.self_attn8 = self_attention()\n",
    "\n",
    "\n",
    "        # All the layers, we gonna need to make the decoder work.\n",
    "        self.layer_norm = nn.LayerNorm(16)\n",
    "        self.softmax = nn.Softmax(-1)\n",
    "\n",
    "        self.latent_downscale = nn.Linear(16, 2)\n",
    "        self.latent_upscale = nn.Linear(2, 16)\n",
    "\n",
    "        self.final_linear_layer = nn.Linear(16, vocab_size) # out_features can be replaced with embedding dimension (at least, here).\n",
    "\n",
    "\n",
    "    def forward(self, Q, K, V, X):\n",
    "\n",
    "        q = self.latent_downscale(Q)\n",
    "        k = self.latent_downscale(K)\n",
    "        v = self.latent_downscale(V)\n",
    "\n",
    "        x = self.latent_downscale(X)\n",
    "        \n",
    "        # getting the contexts from the respective self attention layers in the multi-headed attention block.\n",
    "        context1 = self.self_attn1(q, k, v)\n",
    "        context2 = self.self_attn2(q, k, v)\n",
    "        context3 = self.self_attn3(q, k, v)\n",
    "        context4 = self.self_attn4(q, k, v)\n",
    "\n",
    "        context5 = self.self_attn5(q, k, v)\n",
    "        context6 = self.self_attn6(q, k, v)\n",
    "        context7 = self.self_attn7(q, k, v)\n",
    "        context8 = self.self_attn8(q, k, v)\n",
    "\n",
    "        # adding them up\n",
    "        final_encodings = self.latent_upscale(context1 + context2 + context3 + context5 + context6 + context7 + context8 + x)\n",
    "        final_encodings = self.layer_norm(final_encodings)\n",
    "        logits = self.final_linear_layer(final_encodings)\n",
    "\n",
    "        return logits\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # creating the multi-headed attention block.\n",
    "        self.self_attn1 = self_attention()\n",
    "        self.self_attn2 = self_attention()\n",
    "        self.self_attn3 = self_attention()\n",
    "        self.self_attn4 = self_attention()\n",
    "\n",
    "        self.self_attn5 = self_attention()\n",
    "        self.self_attn6 = self_attention()\n",
    "        self.self_attn7 = self_attention()\n",
    "        self.self_attn8 = self_attention()\n",
    "\n",
    "\n",
    "        # All the layers, we gonna need to make the decoder work.\n",
    "        self.layer_norm = nn.LayerNorm(16)\n",
    "        self.softmax = nn.Softmax(-1)\n",
    "\n",
    "        self.latent_downscale = nn.Linear(16, 2)\n",
    "        self.latent_upscale = nn.Linear(2, 16)\n",
    "\n",
    "        self.final_linear_layer = nn.Linear(16, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, Q, K, V, X, attn_mask=None):\n",
    "        q = self.latent_downscale(Q)\n",
    "        k = self.latent_downscale(K)\n",
    "        v = self.latent_downscale(V)\n",
    "        x = self.latent_downscale(X)\n",
    "\n",
    "        context1 = self.self_attn1(q, k, v, attn_mask)\n",
    "        context2 = self.self_attn2(q, k, v, attn_mask)\n",
    "        context3 = self.self_attn3(q, k, v, attn_mask)\n",
    "        context4 = self.self_attn4(q, k, v, attn_mask)\n",
    "        context5 = self.self_attn5(q, k, v, attn_mask)\n",
    "        context6 = self.self_attn6(q, k, v, attn_mask)\n",
    "        context7 = self.self_attn7(q, k, v, attn_mask)\n",
    "        context8 = self.self_attn8(q, k, v, attn_mask)\n",
    "\n",
    "        combined = torch.cat((context1, context2, context3, context4, context5, context6, context7, context8), 2)\n",
    "        final_encodings = combined + self.latent_upscale(x)\n",
    "        final_encodings = self.layer_norm(final_encodings)\n",
    "        logits = self.final_linear_layer(final_encodings)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\\n\\nembedding_layer = nn.Embedding(vocab_size, embedding_dim).to(device)\\nmodel = Model().to(device)\\nPAD_IDX = vocab_to_index.get(\"<pad>\", 0)  # Ensure this is consistent with your vocab\\nloss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\\n\\n# Assuming: tokenized_lines = open(\"input.txt\").readlines(), vocab_to_idx built\\ndataset = ShakespeareDataset(tokenized_lines, vocab_to_index)\\nloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_batch)\\n\\ndef create_padding_mask(input_ids, pad_idx):\\n    input_ids: (batch, seq_len)\\n    return (input_ids != pad_idx).unsqueeze(1)  # (batch, 1, seq_len)\\n\\nfor epoch in range(10):\\n    total_loss = 0\\n\\n    for inputs, targets in loader:\\n        inputs, targets = inputs.to(device), targets.to(device)\\n        seq_len = inputs.size(1)\\n\\n        # Embeddings + positions\\n        embed = embedding_layer(inputs)  # (batch, seq_len, emb_dim)\\n        pos = positional_encodings(seq_len, embedding_dim, device)\\n        x = embed + pos\\n\\n        # Decoder input\\n        q = k = v = x\\n\\n        attn_mask = create_padding_mask(inputs, PAD_IDX).to(device)  # (batch, 1, seq_len)\\n        logits = model(q, k, v, x, attn_mask=attn_mask)\\n\\n        #print(\"inputs.dtype =\", inputs.dtype)\\n        logits = logits.view(-1, vocab_size)\\n        targets = targets.view(-1).long()  # Ensure targets are Long (int64)\\n\\n        # Ensure logits are float (if any issue with dtype mismatch)\\n        #logits = logits.long()\\n\\n        loss = loss_fn(logits, targets)\\n\\n        optimizer.zero_grad()\\n        loss.backward()\\n        optimizer.step()\\n\\n        total_loss += loss.item()\\n\\n    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\\n'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "embedding_layer = nn.Embedding(vocab_size, embedding_dim).to(device)\n",
    "model = Model().to(device)\n",
    "PAD_IDX = vocab_to_index.get(\"<pad>\", 0)  # Ensure this is consistent with your vocab\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Assuming: tokenized_lines = open(\"input.txt\").readlines(), vocab_to_idx built\n",
    "dataset = ShakespeareDataset(tokenized_lines, vocab_to_index)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "def create_padding_mask(input_ids, pad_idx):\n",
    "    input_ids: (batch, seq_len)\n",
    "    return (input_ids != pad_idx).unsqueeze(1)  # (batch, 1, seq_len)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "\n",
    "    for inputs, targets in loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        seq_len = inputs.size(1)\n",
    "\n",
    "        # Embeddings + positions\n",
    "        embed = embedding_layer(inputs)  # (batch, seq_len, emb_dim)\n",
    "        pos = positional_encodings(seq_len, embedding_dim, device)\n",
    "        x = embed + pos\n",
    "\n",
    "        # Decoder input\n",
    "        q = k = v = x\n",
    "\n",
    "        attn_mask = create_padding_mask(inputs, PAD_IDX).to(device)  # (batch, 1, seq_len)\n",
    "        logits = model(q, k, v, x, attn_mask=attn_mask)\n",
    "\n",
    "        #print(\"inputs.dtype =\", inputs.dtype)\n",
    "        logits = logits.view(-1, vocab_size)\n",
    "        targets = targets.view(-1).long()  # Ensure targets are Long (int64)\n",
    "\n",
    "        # Ensure logits are float (if any issue with dtype mismatch)\n",
    "        #logits = logits.long()\n",
    "\n",
    "        loss = loss_fn(logits, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 7.940370477437973, Accuracy: 0.027560788428527303\n",
      "Epoch 2, Loss: 7.3869781011343, Accuracy: 0.032049023357685655\n",
      "Epoch 3, Loss: 7.332308809757233, Accuracy: 0.03194001373951323\n",
      "Epoch 4, Loss: 7.296758715510368, Accuracy: 0.0322516475507291\n",
      "Epoch 5, Loss: 7.270411431193351, Accuracy: 0.03342948437144514\n",
      "Epoch 6, Loss: 7.249567958116532, Accuracy: 0.03414677297521848\n",
      "Epoch 7, Loss: 7.233850157260894, Accuracy: 0.0344131642911816\n",
      "Epoch 8, Loss: 7.219475389719009, Accuracy: 0.03464604808192234\n",
      "Epoch 9, Loss: 7.206573957204819, Accuracy: 0.03525175296294037\n",
      "Epoch 10, Loss: 7.195132347941398, Accuracy: 0.0358586078742519\n",
      "Epoch 11, Loss: 7.184076891541481, Accuracy: 0.03597830262675416\n",
      "Epoch 12, Loss: 7.173303861618042, Accuracy: 0.03648637200705707\n",
      "Epoch 13, Loss: 7.163004167675972, Accuracy: 0.03665820735215675\n",
      "Epoch 14, Loss: 7.152753324508667, Accuracy: 0.03715173692035023\n",
      "Epoch 15, Loss: 7.1412057501077655, Accuracy: 0.03670240304199979\n",
      "Epoch 16, Loss: 7.1305986392498015, Accuracy: 0.03700626817240846\n",
      "Epoch 17, Loss: 7.118998412489891, Accuracy: 0.036922154787462205\n",
      "Epoch 18, Loss: 7.108022173643112, Accuracy: 0.03703789419378154\n",
      "Epoch 19, Loss: 7.098411940932274, Accuracy: 0.036909857749706135\n",
      "Epoch 20, Loss: 7.089709228277206, Accuracy: 0.03681327361962758\n",
      "Epoch 21, Loss: 7.081952590942382, Accuracy: 0.03705703045008704\n",
      "Epoch 22, Loss: 7.074189414381981, Accuracy: 0.03740250001486856\n",
      "Epoch 23, Loss: 7.0685421192646025, Accuracy: 0.03716055545024574\n",
      "Epoch 24, Loss: 7.062191871404647, Accuracy: 0.03722111494571436\n",
      "Epoch 25, Loss: 7.057174460887909, Accuracy: 0.03722371611278504\n",
      "Epoch 26, Loss: 7.051034489274025, Accuracy: 0.037490679452894256\n",
      "Epoch 27, Loss: 7.045385679602623, Accuracy: 0.03751587850449141\n",
      "Epoch 28, Loss: 7.03972137093544, Accuracy: 0.0374649242288433\n",
      "Epoch 29, Loss: 7.034463306665421, Accuracy: 0.03720915104087908\n",
      "Epoch 30, Loss: 7.0289523190259935, Accuracy: 0.03721037654671818\n",
      "Epoch 31, Loss: 7.023899504542351, Accuracy: 0.03735131707158871\n",
      "Epoch 32, Loss: 7.019742665290832, Accuracy: 0.03795563126623165\n",
      "Epoch 33, Loss: 7.015022631287575, Accuracy: 0.03755672989645973\n",
      "Epoch 34, Loss: 7.010325363874435, Accuracy: 0.03742655135225505\n",
      "Epoch 35, Loss: 7.005456967353821, Accuracy: 0.03776651266380213\n",
      "Epoch 36, Loss: 7.001867972612381, Accuracy: 0.03769336924771778\n",
      "Epoch 37, Loss: 6.998097776770591, Accuracy: 0.03777316960797179\n",
      "Epoch 38, Loss: 6.993785811066627, Accuracy: 0.03768807602638844\n",
      "Epoch 39, Loss: 6.990672571659088, Accuracy: 0.03768922127026599\n",
      "Epoch 40, Loss: 6.98718861758709, Accuracy: 0.03785569308558479\n",
      "Epoch 41, Loss: 6.983492122888565, Accuracy: 0.03790188247221522\n",
      "Epoch 42, Loss: 6.980848659873009, Accuracy: 0.03779433403047733\n",
      "Epoch 43, Loss: 6.977724232673645, Accuracy: 0.03780569533060771\n",
      "Epoch 44, Loss: 6.974599397778511, Accuracy: 0.03789669542864431\n",
      "Epoch 45, Loss: 6.972240419983864, Accuracy: 0.03790065928827971\n",
      "Epoch 46, Loss: 6.97007754445076, Accuracy: 0.0379303055733908\n",
      "Epoch 47, Loss: 6.967239056229591, Accuracy: 0.038088247771956955\n",
      "Epoch 48, Loss: 6.96510828435421, Accuracy: 0.03803464695287403\n",
      "Epoch 49, Loss: 6.963178516030312, Accuracy: 0.03776154347229749\n",
      "Epoch 50, Loss: 6.960793955922127, Accuracy: 0.03777852020866703\n",
      "Epoch 51, Loss: 6.959270941019058, Accuracy: 0.03801384574675467\n",
      "Epoch 52, Loss: 6.957076841592789, Accuracy: 0.038107435204437934\n",
      "Epoch 53, Loss: 6.955287046432495, Accuracy: 0.03828299736254848\n",
      "Epoch 54, Loss: 6.95371936917305, Accuracy: 0.03834298967034556\n",
      "Epoch 55, Loss: 6.95218024790287, Accuracy: 0.03789716108411085\n",
      "Epoch 56, Loss: 6.950196179151535, Accuracy: 0.038031688221381046\n",
      "Epoch 57, Loss: 6.949022520184517, Accuracy: 0.0381164175568847\n",
      "Epoch 58, Loss: 6.948522708415985, Accuracy: 0.038079646018450146\n",
      "Epoch 59, Loss: 6.946164224743843, Accuracy: 0.0384622931398917\n",
      "Epoch 60, Loss: 6.9447456461191175, Accuracy: 0.0385209664166905\n",
      "Epoch 61, Loss: 6.944216429591179, Accuracy: 0.038394666464882904\n",
      "Epoch 62, Loss: 6.942948641777039, Accuracy: 0.0386154340277426\n",
      "Epoch 63, Loss: 6.941559486985207, Accuracy: 0.03831067645864095\n",
      "Epoch 64, Loss: 6.940520917773247, Accuracy: 0.03824357895355206\n",
      "Epoch 65, Loss: 6.939402841329574, Accuracy: 0.03837825640395749\n",
      "Epoch 66, Loss: 6.937911897301674, Accuracy: 0.038852697776746936\n",
      "Epoch 67, Loss: 6.9374604523181915, Accuracy: 0.03863402647024486\n",
      "Epoch 68, Loss: 6.935795511603356, Accuracy: 0.03859040923474822\n",
      "Epoch 69, Loss: 6.93595685839653, Accuracy: 0.0385303787823068\n",
      "Epoch 70, Loss: 6.934385071396828, Accuracy: 0.03848544584761839\n",
      "Epoch 71, Loss: 6.9336284267902375, Accuracy: 0.03859824564191513\n",
      "Epoch 72, Loss: 6.932880992889404, Accuracy: 0.03905344568192959\n",
      "Epoch 73, Loss: 6.932253007292747, Accuracy: 0.038670888680499046\n",
      "Epoch 74, Loss: 6.9314898246526715, Accuracy: 0.038890017810626884\n",
      "Epoch 75, Loss: 6.930277853012085, Accuracy: 0.03900259813060984\n",
      "Epoch 76, Loss: 6.930093324184417, Accuracy: 0.03864121242018882\n",
      "Epoch 77, Loss: 6.9291495823860165, Accuracy: 0.03895384692470543\n",
      "Epoch 78, Loss: 6.929432510137558, Accuracy: 0.03894817407184746\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[166], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size), targets\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 32\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Accuracy\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embedding_layer = nn.Embedding(vocab_size, embedding_dim).to(device)\n",
    "#model = Model().to(device)\n",
    "PAD_IDX = vocab_to_index.get(\"<pad>\", 0)  # Ensure this is consistent with your vocab\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Assuming: tokenized_lines = open(\"input.txt\").readlines(), vocab_to_idx built\n",
    "dataset = ShakespeareDataset(tokenized_lines, vocab_to_index)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "def create_padding_mask(input_ids, pad_idx):\n",
    "    input_ids: (batch, seq_len)\n",
    "    return (input_ids != pad_idx).unsqueeze(1)  # (batch, 1, seq_len)\n",
    "\n",
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "\n",
    "    for inputs, targets in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Get embeddings\n",
    "        input_embeddings = embedding_layer(inputs)\n",
    "        pos_enc = positional_encodings(input_embeddings.size(1), embedding_dim, device)\n",
    "        input_with_pos = input_embeddings + pos_enc\n",
    "\n",
    "        logits = model(input_with_pos, input_with_pos, input_with_pos, input_with_pos)\n",
    "        \n",
    "        loss = loss_fn(logits.view(-1, vocab_size), targets.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accuracy\n",
    "        predicted = torch.argmax(logits, dim=-1)\n",
    "        correct = (predicted == targets).float()\n",
    "        mask = (targets != PAD_IDX).float()\n",
    "        accuracy = (correct * mask).sum() / mask.sum()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_accuracy += accuracy.item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    avg_accuracy = total_accuracy / len(loader)\n",
    "    #print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.4f}\")\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss}, Accuracy: {avg_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.522105798125267"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([11228,  6594,     0,     0,     0,     0,     0,     0,     0])\n",
      "Target: tensor([ 6594, 10941,     0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "for x, y in loader:\n",
    "    print(\"Input:\", x[0])\n",
    "    print(\"Target:\", y[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3021, -0.7764,  0.5359,  ...,  0.4725, -0.1413, -0.4906],\n",
       "         [-0.2999, -0.7784,  0.5250,  ...,  0.4617, -0.1388, -0.4856],\n",
       "         [-0.1239, -0.8907,  0.3899,  ...,  0.2340, -0.0455, -0.6259],\n",
       "         ...,\n",
       "         [-0.7696,  0.6514,  0.5770,  ...,  0.4585, -0.3713,  0.2837],\n",
       "         [-0.7342,  0.7084,  0.5475,  ...,  0.4854, -0.2788,  0.3135],\n",
       "         [-0.6220,  0.6547,  0.7251,  ...,  0.6407, -0.2005,  0.0632]],\n",
       "\n",
       "        [[ 0.0082,  0.3777,  0.9690,  ...,  0.7708,  0.2903, -0.8089],\n",
       "         [-0.3768, -0.7505,  0.2232,  ...,  0.1718, -0.1880, -0.1934],\n",
       "         [-0.4395, -0.7145,  0.1079,  ...,  0.0551, -0.2404, -0.0471],\n",
       "         ...,\n",
       "         [-0.5848, -0.6626,  0.1160,  ..., -0.0113, -0.4363,  0.0604],\n",
       "         [-0.7534, -0.5014,  0.1725,  ...,  0.0251, -0.5941,  0.2111],\n",
       "         [-0.8981, -0.0929,  0.4464,  ...,  0.2271, -0.7270,  0.2356]],\n",
       "\n",
       "        [[ 0.0989, -0.8582,  0.7987,  ...,  0.5686,  0.1130, -1.1362],\n",
       "         [-0.3044, -0.7957,  0.2549,  ...,  0.1787, -0.1435, -0.3053],\n",
       "         [-0.2973, -0.8377,  0.2995,  ...,  0.1594, -0.1960, -0.3834],\n",
       "         ...,\n",
       "         [-0.9318,  0.0078,  0.2912,  ...,  0.0503, -0.7499,  0.3922],\n",
       "         [-1.0087,  0.3645,  0.4244,  ...,  0.3113, -0.6641,  0.5286],\n",
       "         [-0.8970,  0.3550,  0.6991,  ...,  0.5718, -0.5759,  0.2276]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.6875, -0.3424,  0.5777,  ...,  0.7182, -0.2935,  0.0460],\n",
       "         [-0.3836, -0.5597,  0.8676,  ...,  0.9042, -0.1201, -0.5397],\n",
       "         [-0.1945, -0.6273,  1.0076,  ...,  0.9567, -0.0138, -0.8674],\n",
       "         ...,\n",
       "         [-0.9533,  0.3409,  0.4632,  ...,  0.2868, -0.6625,  0.4159],\n",
       "         [-0.9259,  0.5580,  0.5052,  ...,  0.4260, -0.5174,  0.4671],\n",
       "         [-0.7594,  0.5638,  0.7454,  ...,  0.6516, -0.3683,  0.1444]],\n",
       "\n",
       "        [[ 0.0765, -0.1718,  1.2633,  ...,  1.1095,  0.2667, -1.2094],\n",
       "         [-0.3927, -0.7154,  0.2776,  ...,  0.2625, -0.1721, -0.1902],\n",
       "         [-0.5900, -0.5668,  0.0273,  ...,  0.0390, -0.3112,  0.2166],\n",
       "         ...,\n",
       "         [-0.9210,  0.4541,  0.5126,  ...,  0.3577, -0.5920,  0.3939],\n",
       "         [-0.8823,  0.6089,  0.5230,  ...,  0.4511, -0.4574,  0.4328],\n",
       "         [-0.7326,  0.5868,  0.7444,  ...,  0.6535, -0.3338,  0.1284]],\n",
       "\n",
       "        [[ 0.0765, -0.1718,  1.2633,  ...,  1.1095,  0.2667, -1.2094],\n",
       "         [-0.6810, -0.2781,  0.3987,  ...,  0.5803, -0.2334,  0.2007],\n",
       "         [-0.0915, -0.8637,  0.5424,  ...,  0.4138,  0.0067, -0.7411],\n",
       "         ...,\n",
       "         [-0.9546, -0.2114,  0.1222,  ...,  0.1357, -0.6153,  0.5837],\n",
       "         [-1.0166,  0.3108,  0.3995,  ...,  0.2759, -0.6901,  0.5331],\n",
       "         [-0.8987,  0.3504,  0.6974,  ...,  0.5692, -0.5790,  0.2287]]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "<start> it he friends, it poor he our my am this? at that part but you a is courage, you my have the peruse so with and that confessor, and i care when deliberate nothing you to take not mortal in and like to empty is that him come or else\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def generate_sequence(model, start_text, vocab_to_idx, idx_to_vocab, embedding_layer, device, max_len=50):\n",
    "    model.eval()  # Evaluation mode\n",
    "    start_tokens = start_text.lower().split()\n",
    "\n",
    "    # Convert words to indices\n",
    "    input_ids = [vocab_to_idx.get(word, vocab_to_idx[\"<pad>\"]) for word in start_tokens]\n",
    "    generated = torch.tensor(input_ids, dtype=torch.long, device=device).unsqueeze(0)  # (1, seq_len)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        seq_len = generated.size(1)\n",
    "\n",
    "        # Recalculate positional encodings each time\n",
    "        pos = positional_encodings(seq_len, embedding_layer.embedding_dim, device)\n",
    "        input_embed = embedding_layer(generated) + pos\n",
    "\n",
    "        # Attention mask\n",
    "        attn_mask = create_padding_mask(generated, vocab_to_idx[\"<pad>\"]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q = k = v = input_embed\n",
    "            logits = model(q, k, v, input_embed, attn_mask)\n",
    "\n",
    "        # Sample next token\n",
    "        logits = logits[:, -1, :]  # Get last token's logits\n",
    "        temperature = 0.7\n",
    "        probs = torch.softmax(logits / temperature, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)  # Shape: (1, 1)\n",
    "\n",
    "        # Stop if end token\n",
    "        token_id = next_token.item()\n",
    "        if idx_to_vocab.get(token_id, \"\") == \"<end>\":\n",
    "            break\n",
    "\n",
    "        # Append next token\n",
    "        generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "    # Convert generated indices back to words\n",
    "    generated_text = ' '.join([idx_to_vocab.get(idx.item(), \"<unk>\") for idx in generated.squeeze()])\n",
    "    return generated_text\n",
    "\n",
    "# Example of inference usage:\n",
    "start_text = \"<start>\"  # Starting text for generation\n",
    "generated_text = generate_sequence(\n",
    "    model=model, \n",
    "    start_text=start_text, \n",
    "    vocab_to_idx=vocab_to_index, \n",
    "    idx_to_vocab={index: word for word, index in vocab_to_index.items()}, \n",
    "    embedding_layer=embedding_layer, \n",
    "    device=device,\n",
    "    max_len=50  # Limit generated sequence length\n",
    ")\n",
    "\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
