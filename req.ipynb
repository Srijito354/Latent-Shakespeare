{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Extract Conversations\n",
    "# -----------------------------\n",
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "def extract_conversations(text):\n",
    "    lines = text.split('\\n')\n",
    "    conversations = []\n",
    "    for i in range(len(lines)-1):\n",
    "        if lines[i].startswith(\"First Citizen\") and lines[i+1].startswith(\"All\"):\n",
    "            user_line = lines[i].split(\"First Citizen\", 1)[-1].strip()\n",
    "            bot_line = lines[i+1].split(\"All\", 1)[-1].strip()\n",
    "            if user_line and bot_line:\n",
    "                conversations.append((user_line, bot_line))\n",
    "    return conversations\n",
    "\n",
    "conversations = extract_conversations(raw_text)\n",
    "print(f\"Extracted {len(conversations)} user-bot pairs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, conversations, stoi, block_size=30):\n",
    "        self.stoi = stoi\n",
    "        self.block_size = block_size\n",
    "        self.data = []\n",
    "\n",
    "        for user, bot in conversations:\n",
    "            user_tokens = [stoi[w] for w in user.lower().split() if w in stoi]\n",
    "            bot_tokens = [stoi[w] for w in bot.lower().split() if w in stoi]\n",
    "            tokens = user_tokens + [stoi['<sep>']] + bot_tokens\n",
    "            if len(tokens) >= 2:\n",
    "                self.data.append(torch.tensor(tokens, dtype=torch.long))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx][:-1]\n",
    "        y = self.data[idx][1:]\n",
    "        return x, y\n",
    "\n",
    "# Add special <sep> token if missing\n",
    "if '<sep>' not in stoi:\n",
    "    sep_idx = len(stoi)\n",
    "    stoi['<sep>'] = sep_idx\n",
    "    itos[sep_idx] = '<sep>'\n",
    "    vocab_size += 1\n",
    "\n",
    "conversation_dataset = ConversationDataset(conversations, stoi)\n",
    "conversation_loader = DataLoader(conversation_dataset, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Training loop\n",
    "# -----------------------------\n",
    "def train(model, loader, optimizer, device='cpu', epochs=5):\n",
    "    model.train()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for step, (x, y) in enumerate(loader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits, loss = model(x, targets=y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if step % 50 == 0:\n",
    "                print(f\"Epoch {epoch} Step {step} Loss: {loss.item():.4f}\")\n",
    "\n",
    "        print(f\"Epoch {epoch} Average Loss: {total_loss / len(loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1. Load data & build vocab\n",
    "# -----------------------------\n",
    "\n",
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read().lower()\n",
    "\n",
    "sentences = text.split('\\n')  # Split into lines/sentences\n",
    "words = text.split()          # Word-level tokenization\n",
    "\n",
    "unique_words = sorted(set(words))\n",
    "stoi = {word: idx for idx, word in enumerate(unique_words)}\n",
    "itos = {idx: word for word, idx in stoi.items()}\n",
    "vocab_size = len(stoi)\n",
    "embedding_dim = 16\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}, Total sentences: {len(sentences)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Positional encoding\n",
    "# -----------------------------\n",
    "\n",
    "def positional_encodings(sequence_length, embedding_size):\n",
    "    pe = torch.zeros(sequence_length, embedding_size)\n",
    "    for pos in range(sequence_length):\n",
    "        for i in range(embedding_size):\n",
    "            if i % 2 == 0:\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/embedding_size)))\n",
    "            else:\n",
    "                pe[pos, i] = math.cos(pos / (10000 ** ((2 * i)/embedding_size)))\n",
    "    return pe\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Dataset\n",
    "# -----------------------------\n",
    "\n",
    "class WordLevelDataset(Dataset):\n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = [s for s in sentences if len(s.split()) >= 2]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx]\n",
    "\n",
    "dataset = WordLevelDataset(sentences)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Shared embedding\n",
    "# -----------------------------\n",
    "\n",
    "embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "def embedding_gen(sentence):\n",
    "    words = sentence.lower().split()\n",
    "    indices = [stoi[word] for word in words if word in stoi]\n",
    "    input_tensor = torch.LongTensor(indices)\n",
    "    input_embeddings = embedding_layer(input_tensor)\n",
    "    return input_embeddings, input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = Model().to(device)\n",
    "optimizer = torch.optim.Adam(list(model.parameters()) + list(embedding_layer.parameters()), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn.to(device)\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        sentence = batch[0]\n",
    "        input_embeddings, input_tensor = embedding_gen(sentence)\n",
    "        input_embeddings = input_embeddings.to(device)\n",
    "        input_tensor = input_tensor.to(device)\n",
    "\n",
    "        if len(input_tensor) < 2:\n",
    "            continue  # skip short lines\n",
    "\n",
    "        pos_enc = positional_encodings(input_embeddings.size(0), embedding_dim)\n",
    "        pos_enc = pos_enc.to(device)\n",
    "        input_combined = input_embeddings + pos_enc\n",
    "        input_combined = input_combined.to(device)\n",
    "        input_tensor = input_tensor.to(device)\n",
    "\n",
    "        # Forward\n",
    "        logits = model(input_combined, input_combined, input_combined, input_combined)\n",
    "        #logits = torch.matmul(output, embedding_layer.weight.T)\n",
    "\n",
    "        embedding_layer.to(device)\n",
    "\n",
    "        # Predict next word embeddings\n",
    "        target = input_tensor[1:]\n",
    "        pred = logits[0, :-1]\n",
    "\n",
    "        loss = loss_fn(pred, embedding_layer(target))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Avg Loss: {total_loss / len(loader):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
